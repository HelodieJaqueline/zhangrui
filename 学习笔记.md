# 学习笔记、日常积累

## 计算机基础
### 网络

#### 四层协议/七层协议



#### TCP/IP



#### HTTP



##### 三次握手&四次挥手

#### BIO、NIO、AIO

客户端->服务端是同步还是异步

IO是阻塞还是非阻塞

##### BIO

##### NIO

多路复用

**select**

轮询

支持的连接数是有限(1024)

**epoll**

同步非阻塞模型

* 伪异步，socket就绪后还是需要客户端去请求才能拿到结果。
* 异步:服务端数据准备好后直接返回发送给客户端

事件驱动

##### AIO

### 操作系统
#### Linux
##### 常用命令




#### CPU

#### 内存

#### 磁盘

#### 线程

#### 进程

#### 死锁

### 数据结构和算法

算法题目积累: https://github.com/HelodieJaqueline/zhangrui.git

#### 数据结构

##### 数组

##### 链表

##### 队列

##### 字符串

##### 树

#### 算法

##### 排序

##### 动态规划

#### 查找

## 语言基础

### 面向对象设计七大原则

**1. 单一职责原则（Single Responsibility Principle）**

每一个类应该专注于做一件事情。

**2. 里氏替换原则（Liskov Substitution Principle）**

超类存在的地方，子类是可以替换的。

**3. 依赖倒置原则（Dependence Inversion Principle）**

实现尽量依赖抽象，不依赖具体实现。

**4. 接口隔离原则（Interface Segregation Principle）**

应当为客户端提供尽可能小的单独的接口，而不是提供大的总的接口。

**5. 迪米特法则（Law Of Demeter）**

又叫最少知识原则，一个软件实体应当尽可能少的与其他实体发生相互作用。

**6. 开闭原则（Open Close Principle）**

面向扩展开放，面向修改关闭。

**7. 组合/聚合复用原则（Composite/Aggregate Reuse Principle CARP）**

尽量使用合成/聚合达到复用，尽量少用继承。原则： 一个类中有另一个类的对象。

### 语言特性

#### 出现在Java程序中的finally代码块是否一定会执行？

当遇到下面情况不会执行。 

1.  当程序在进入try语句块之前就出现异常时会直接结束。 
2.  当程序在try块中强制退出时，如使用System.exit(0)，也不会执行finally块中的代码。 

 其它情况下

* 在try/catch/finally语句执行的时候，try块先执行，当有异常发生，catch和finally进行处理后程序就结束了 

* 当没有异常发生，在执行完finally中的代码后，后面代码会继续执行。

* 值得注意的是，当try/catch语句块中有return时，finally语句块中的代码会在return之前执行。

* 如果try/catch/finally块中都有return语句，finally块中的return语句会覆盖try/catch模块中的return语句。

#### 关键字static的作用是什么？

static的主要作用有两个： 

1.  为某种特定数据类型或对象分配与创建对象个数无关的单一的存储空间。 
2.  使得某个方法或属性与类而不是对象关联在一起，即在不创建对象的情况下可通过类直接调用方法或使用类的属性。 

 具体而言static又可分为4种使用方式： 

1.  修饰成员变量。用static关键字修饰的静态变量在内存中只有一个副本。只要静态变量所在的类被加载，这个静态变量就会被分配空间，可以使用''类.静态变量''和''对象.静态变量''的方法使用。 
2.  修饰成员方法。static修饰的方法无需创建对象就可以被调用。static方法中不能使用this和super关键字，不能调用非static方法，只能访问所属类的静态成员变量和静态成员方法。 
3.  修饰代码块。JVM在加载类的时候会执行static代码块。static代码块常用于初始化静态变量。static代码块只会被执行一次。 
4.  修饰内部类。static内部类可以不依赖外部类实例对象而被实例化。静态内部类不能与外部类有相同的名字，不能访问普通成员变量，只能访问外部类中的静态成员和静态成员方法。

#### 为什么要把String设计为不变量？ 

1.  节省空间：字符串常量存储在JVM的字符串池中可以被用户共享。 
2.  提高效率:String会被不同线程共享，是线程安全的。在涉及多线程操作中不需要同步操作。 
3.  安全：String常被用于用户名、密码、文件名等使用，由于其不可变，可避免黑客行为对其恶意修改。

#### Java反射机制是什么？ 

 Java反射机制是指在程序的运行过程中可以构造任意一个类的对象、获取任意一个类的成员变量和成员方法、获取任意一个对象所属的类信息、调用任意一个对象的属性和方法。反射机制使得Java具有动态获取程序信息和动态调用对象方法的能力。可以通过以下类调用反射API。 

-  Class类：可获得类属性方法 
-  Field类：获得类的成员变量 
-  Method类：获取类的方法信息 
-  Construct类：获取类的构造方法等信息

#### 简述注解 

 Java 注解用于为 Java 代码提供元数据。作为元数据，注解不直接影响你的代码执行，但也有一些类型的注解实际上可以用于这一目的。 

 其可以用于提供信息给编译器，在编译阶段时给软件提供信息进行相关的处理，在运行时处理写相应代码，做对应操作。 

####  简述元注解 

 元注解可以理解为注解的注解，即在注解中使用，实现想要的功能。其具体分为： 

-  @Retention: 表示注解存在阶段是保留在[源码]()，还是在字节码（类加载）或者运行期（JVM中运行）。 
-  @Target：表示注解作用的范围。 
-  @Documented：将注解中的元素包含到 Javadoc 中去。 
-  @Inherited：一个被@Inherited注解了的注解修饰了一个父类，如果他的子类没有被其他注解修饰，则它的子类也继承了父类的注解。 
-  @Repeatable：被这个元注解修饰的注解可以同时作用一个对象多次，但是每次作用注解又可以代表不同的含义。







### 集合框架

#### List

##### ArrayList

底层使用数组实现，查询快(下标访问)，但是数组的容量每次是指定的，扩容变成原来的1.5倍。插入修改和删除操作慢。遍历删除的话可以重尾部遍历，减少移动。

##### LinkedList

底层使用链表实现，插入，删除快，且不用初始化容量

##### 线程安全的List

* Vector

  方法使用synchronized修饰，性能差

* Collections.synchronizedList

  也是加synchronized修饰，比vector灵活点

* CopyOnWriteArrayList

  使用ReentrantLock在写的时候加锁，读的时候没有加锁。

  put操作的时候底层实现添加的原理是先copy出一个容器(可以简称副本)，再往新的容器里添加这个新的数据，最后把新的容器的引用地址赋值给了之前那个旧的的容器地址，但是在添加这个数据的期间，其他线程如果要去读取数据，仍然是读取到旧的容器里的数据。

  

#### Set

##### HashSet

##### TreeSet

##### LinkedHashSet

##### 线程安全的Set

#### Map

##### HashMap

##### LinkedHashMap

##### ConcurrentHashMap

#### Queue

### 异常处理

#### 异常分类

##### 受检异常

##### 非受检异常

### 多线程并发

#### 多线程基础

##### 线程的生命周期

线程的6种状态(NEW、RUNNABLE、BLOCKED、WAITING、TIME_WAITING、TERMINATED)

* NEW：初始状态，线程被构建，但是还没有调用 start 方法RUNNABLED：运行状态，JAVA 线程把操作系中的就绪和运行两种状态统一称为“运行中”
* BLOCKED：阻塞状态，表示线程进入等待状态,也就是线程因为某种原因放弃了 CPU 使用权，阻塞也分为几种情况
  ➢ 等待阻塞：运行的线程执行 wait 方法，jvm 会把当前线程放入到等待队列
  ➢ 同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被其他线程锁占用了，那么 jvm 会把当前的线程放入到锁池中
  ➢ 其他阻塞：运行的线程执行 Thread.sleep 或者 t.join 方法，或者发出了 I/O 请求时，JVM 会把当前线程设置为阻塞状态，当 sleep 结束、join 线程终止、io 处理完毕则线程恢复
* TIME_WAITING：超时等待状态，超时以后自动返回
* TERMINATED：终止状态，表示当前线程执行完毕

##### 线程的启动

线程的启动，也就是调用start()方法去启动一个线程，当 run 方法中的代码执行完毕以后，线程的生命周期也将终止。调用 start 方法的语义是当前线程告诉 JVM，启动调用 start 方法的线程。

##### 线程的终止

线程的终止，并不是简单的调用 stop 命令去。虽然 api 仍然可以调用，但是和其他的线程控制方法如 suspend、resume 一样都是过期了的不建议使用，就拿 stop 来说，stop 方法在结束一个线程时并不会保证线程的资源正常释放，因此会导致程序可能出现一些不确定的状态。要优雅的去中断一个线程，在线程中提供了一个 interrupt方法。

##### interrupt() 方法

当其他线程通过调用当前线程的 interrupt 方法，表示向当前线程打个招呼，告诉他可以中断线程的执行了，至于什么时候中断，取决于当前线程自己。线程通过检查自身是否被中断来进行相应，可以通过isInterrupted()来判断是否被中断。这种通过标识位或者中断操作的方式能够使线程在终止时有机会去清理资源，而不是武断地将线程停止，因此这种终止线程的做法显得更加安全和优雅。

**thread.interrupt()**方法实际就是设置一个 interrupted 状态标识为 true、并且通过ParkEvent 的 unpark 方法来唤醒线程。

##### Thread.interrupted

通过 interrupt，设置了一个标识告诉线程可 以 终 止 了 ， 线 程 中 还 提 供 了 静 态 方 法Thread.interrupted()对设置中断标识的线程复位。

##### 其他的线程复位

除了通过 Thread.interrupted 方法对线程中断标识进行复位 以 外 ， 还 有 一 种 被 动 复 位 的 场 景 ， 就 是 对 抛 出InterruptedException 异 常 的 方 法 ， 在InterruptedException 抛出之前，JVM 会先把线程的中断标识位清除，然后才会抛出 InterruptedException，这个时候如果调用 isInterrupted 方法，将会返回 false。

##### 为什么 Object.wait、Thread.sleep 和 Thread.join 都 会 抛 出InterruptedException? 

这几个方法有一个共同点，都是属于**阻塞的方法**，而阻塞方法的释放会取决于一些外部的事件，但是**阻塞方法可能因为等不到外部的触发事件而导致无法终止**，**所以它允许一个线程请求自己来停止它正在做的事情**。当一个方法抛出 InterruptedException 时，它是在告诉调用者如果执行该方法的线程被中断，它会尝试停止正在做的事情并且通过抛出 InterruptedException 表示提前返回。所以，这个异常的意思是表示一个阻塞被其他线程中断了。然后，由于线程调用了 interrupt()中断方法，那么Object.wait、Thread.sleep 等被阻塞的线程被唤醒以后会通过 is_interrupted 方法判断中断标识的状态变化，如果发现中断标识为 true，则先清除中断标识，然后抛出InterruptedException

需要注意的是，InterruptedException 异常的抛出并不意味着线程必须终止，而是提醒当前线程有中断的操作发生，至于接下来怎么处理取决于线程本身，比如
1. 直接捕获异常不做任何处理
2. 将异常往外抛出
3. 停止当前线程，并打印异常信息

##### volatile  的作用

volatile 可以使得在**多处理器**环境下保证了**共享变量**的**可见性**。

那么到底什么是可见性呢？在单线程的环境下，如果向一个变量先写入一个值，然后在没有写干涉的情况下读取这个变量的值，那这个时候读取到的这个变量的值应该是之前写入的那个值。这本来是一个很正常的事情。但是在多线程环境下，读和写发生在不同的线程中的时候，可能会出现：读线程不能及时的读取到其他线程写入的最新的值。这就是所谓的可见性为了实现跨线程写入的内存可见性，必须使用到一些机制来实现。而 volatile 就是这样一种机制

##### volatile  关键字是如何保证可见性的？

在修改带有 volatile 修饰的成员变量时，会多一个 lock 指令。lock是一种控制指令，在多处理器环境下，lock 汇编指令可以基于总线锁或者缓存锁的机制来达到可见性的一个效果。

##### 从硬件层面了解可见性的本质

一台计算机中最核心的组件是 CPU、内存、以及 I/O 设备。在整个计算机的发展历程中，除了 CPU、内存以及 I/O 设备不断迭代升级来提升计算机处理性能之外，还有一个非常核心的矛盾点，就是这三者在处理速度的差异。CPU 的计算速度是非常快的，内存次之、最后是 IO 设备比如磁盘。而在绝大部分的程序中，一定会存在内存访问，有些可能还会存在 I/O 设备的访问为了提升计算性能，CPU 从单核升级到了多核甚至用到了超线程技术最大化提高 CPU 的处理性能，但是仅仅提升CPU 性能还不够，如果后面两者的处理性能没有跟上，意味着整体的计算效率取决于最慢的设备。为了平衡三者的速度差异，最大化的利用 CPU 提升性能，从硬件、操作系统、编译器等方面都做出了很多的优化:
1. CPU 增加了高速缓存
2. 操作系统增加了进程、线程。通过 CPU 的时间片切换最
大化的提升 CPU 的使用率
3. 编译器的指令优化，更合理的去利用好 CPU 的高速缓存
然后每一种优化，都会带来相应的问题，而这些问题也是
导致线程安全性问题的根源。为了了解前面提到的可见性
问题的本质，我们有必要去了解这些优化的过程.

CPU 高速缓存线程是 CPU 调度的最小单元，线程设计的目的最终仍然是更充分的利用计算机处理的效能，但是绝大部分的运算任务不能只依靠处理器“计算”就能完成，处理器还需要与内存交互，比如读取运算数据、存储运算结果，这个 I/O 操作是很难消除的。而由于计算机的存储设备与处理器的运算速度差距非常大，所以现代计算机系统都会增加一层读写速度尽可能接近处理器运算速度的高速缓存来作为内存和处理器之间的缓冲：将运算需要使用的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步到内存之中。**通过高速缓存的存储交互很好的解决了处理器与内存的速度矛盾，但是也为计算机系统带来了更高的复杂度**，**因为它引入了一个新的问题，缓存一致性。**

##### 什么叫缓存一致性？

首先，有了高速缓存的存在以后，每个 CPU 的处理过程是，先将计算需要用到的数据缓存在 CPU 高速缓存中，在 CPU进行计算时，直接从高速缓存中读取数据并且在计算完成之后写入到缓存中。在整个运算过程完成后，再把缓存中的数据同步到主内存。由于在多CPU中，每个线程可能会运行在不同的CPU内，并且每个线程拥有自己的高速缓存。同一份数据可能会被缓存到多个 CPU 中，如果在不同 CPU 中运行的不同线程看到同一份内存的缓存值不一样就会存在缓存不一致的问题为了解决缓存不一致的问题，在 CPU 层面做了很多事情，主要提供了两种解决办法
1. 总线锁
2. 缓存锁

##### 总线锁和缓存锁

总线锁，简单来说就是，在多 cpu 下，当其中一个处理器要对共享内存进行操作的时候，在总线上发出一个 LOCK#信号，这个信号使得其他处理器无法通过总线来访问到共享内存中的数据，总线锁定把 CPU 和内存之间的通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，这种机制显然是不合适的如何优化呢？最好的方法就是控制锁的保护粒度，我们只需要保证对于被多个 CPU 缓存的同一份数据是一致的就行。所以引入了缓存锁，它核心机制是基于缓存一致性协议来实现的。

##### 缓存一致性协议

为了达到数据访问的一致，需要各个处理器在访问缓存时遵循一些协议，在读写时根据协议来操作，常见的协议有MSI，MESI，MOSI 等。最常见的就是 MESI 协议。

#####  MESI

MESI 表示缓存行的四种状态，分别是
1. M(Modify) 表示共享数据只缓存在当前 CPU 缓存中，并且是被修改状态，也就是缓存的数据和主内存中的数
据不一致
2. E(Exclusive) 表示缓存的独占状态，数据只缓存在当前CPU 缓存中，并且没有被修改
3. S(Shared) 表示数据可能被多个 CPU 缓存，并且各个缓存中的数据和主内存数据一致
4. I(Invalid) 表示缓存已经失效在 MESI 协议中，每个缓存的缓存控制器不仅知道自己的读写操作，而且也监听(snoop)其它 Cache 的读写操作

对于 MESI 协议，从 CPU 读写角度来说会遵循以下原则：CPU 读请求：缓存处于 M、E、S 状态都可以被读取，I 状态 CPU 只能从主存中读取数据CPU 写请求：缓存处于 M、E 状态才可以被写。对于 S 状态的写，需要将其他 CPU 中缓存行置为无效才可写使用总线锁和缓存锁机制之后。

##### 总结可见性的本质

由于 CPU 高速缓存的出现使得 如果多个 cpu 同时缓存了相同的共享数据时，可能存在可见性问题。也就是 CPU0 修改了自己本地缓存的值对于 CPU1 不可见。不可见导致的后果是 CPU1 后续在对该数据进行写入操作时，是使用的脏数据。使得数据最终的结果不可预测。很多同学肯定希望想在代码里面去模拟一下可见性的问题，实际上，这种情况很难模拟。因为我们无法让某个线程指定某个特定 CPU，这是系统底层的算法， JVM 应该也是没法控制的。还有最重要的一点，就是你无法预测 CPU 缓存什么时候会把值传给主存，可能这个时间间隔非常短，短到你无法观察到。最后就是线程的执行的顺序问题，因为多线程你无法控制哪个线程的某句代码会在另一个线程的某句代码后面马上执行。所以我们只能基于它的原理去了解这样一个存在的客观事实了解到这里，大家应该会有一个疑问，刚刚不是说基于缓存一致性协议或者总线锁能够达到缓存一致性的要求吗？为什么还需要加 volatile 关键字？或者说为什么还会存在可见性问题呢？

##### MESI 优化带来的可见性问题

MESI 协议虽然可以实现缓存的一致性，但是也会存在一些问题。就是各个 CPU 缓存行的状态是通过消息传递来进行的。如果 CPU0 要对一个在缓存中共享的变量进行写入，首先需要发送一个失效的消息给到其他缓存了该数据的 CPU。并且要等到他们的确认回执。CPU0 在这段时间内都会处于阻塞状态。为了避免阻塞带来的资源浪费。在 cpu 中引入了 Store Bufferes。

CPU0 只需要在写入共享数据时，直接把数据写入到 storebufferes 中，同时发送 invalidate 消息，然后继续去处理其他指令。当收到其他所有CPU发送了invalidate acknowledge消息时，再将 store bufferes 中的数据数据存储至 cache line中。最后再从缓存行同步到主内存。

#### JMM

JMM 全称是 Java Memory Model. 什么是 JMM 呢？

JMM 属于语言级别的抽象内存模型，可以简单理解为**对硬件模型的抽象**，它定义了**共享内存中多线程程序读写操作的行为规范**：在虚拟机中把共享变量存储到内存以及从内存中取出共享变量的底层实现细节通过这些规则来规范对内存的读写操作从而保证指令的正确性，它解决了 CPU 多级缓存、处理器优化、指令重排序导致的内存访问问题，保证了并发场景下的可见性。

JMM 抽象模型分为主内存、工作内存；主内存是所有线程共享的，一般是实例对象、静态字段、数组对象等存储在堆内存中的变量。工作内存是每个线程独占的，线程对变量的所有操作都必须在工作内存中进行，不能直接读写主内存中的变量，线程之间的共享变量值的传递都是基于主内存来完成。

Java 内存模型底层实现可以简单的认为：通过内存屏障(memory barrier)禁止重排序，即时编译器根据具体的底层体系架构，将这些内存屏障替换成具体的 CPU 指令。对于编译器而言，内存屏障将限制它所能做的重排序优化。而对于处理器而言，内存屏障将会导致缓存的刷新操作。比如，对于 volatile，编译器将在 volatile 字段的读写操作前后各插入一些内存屏障。

##### JMM  是如何解决可见性有序性问题的

简单来说，JMM 提供了一些禁用缓存以及进制重排序的方法，来解决可见性和有序性问题。这些方法大家都很熟悉：**volatile、synchronized、final；**

##### JMM  如何解决顺序一致性问题

**重排序问题**

为了提高程序的执行性能，编译器和处理器都会对指令做重排序，其中处理器的重排序在前面已经分析过了。所谓
的重排序其实就是指执行的指令顺序。编译器的重排序指的是程序编写的指令在编译之后，指令可能会产生重排序来优化程序的执行性能。

##### HappenBefore

它的意思表示的是前一个操作的结果对于后续操作是可见的，所以它是一种表达多个线程之间对于内存的可见性。所以我们可以认为在 JMM 中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作必须要存在happens-before 关系。这两个操作可以是同一个线程，也可以是不同的线程。

**JMM  中有哪些方法建立  happens- -before规则**

程序顺序规则

1. 一个线程中的每个操作，happens-before 于该线程中的任意后续操作; 可以简单认为是 as-if-serial。 单个线程中的代码顺序不管怎么变，对于结果来说是不变的。
2. volatile 变量规则，对于 volatile 修饰的变量的写的操作，一定 happen-before 后续对于 volatile 变量的读操作；
3. 传递性规则，如果 1 happens-before 2; 3happens-before 4; 那么传递性规则表示: 1 happens-before 4;
4. start 规则，如果线程 A 执行操作 ThreadB.start(),那么线程 A 的 ThreadB.start()操作 happens-before 线程 B 中的任意操作。
5. join 规则，如果线程 A 执行操作 ThreadB.join()并成功返回，那么线程 B 中的任意操作 happens-before 于线程A 从 ThreadB.join()操作成功返回。
6. 监视器锁的规则，对一个锁的解锁，happens-before 于随后对这个锁的加锁。

#### Volatile

如果一个变量被声明为volatile，Java线程模型会确保所有线程看到这个变量的值是一致的(缓存一致性协议MESI)

##### CPU术语相关

volatile变量编译后会多出一个lock指令，会使处理器做以下两件事:

* 将当前处理器的缓存行写回到系统内存(主内存)
* 这个写回内存的操作会使在其他CPU中缓存了该内存地址的数据失效

#### Synchronized

基于JVM实现的锁

1、加在代码块上时，是在同步代码块开始的地方插入**monitorenter**指令，在结束的地方(正常结束和异常处)插入**monitoexit**指令。

2、加在方法上是有一个ACC_ACKNOWLEDGE

##### 锁升级

1.6以后对synchroized进行了优化，加入了偏向锁和轻量级锁

**无锁**

**偏向锁**

加锁和解锁不需要额外的开销，如果线程之间存在竞争，会带来额外的锁撤销开销。所有适合只有一个线程访问同步块

偏向锁会记录对象头中的偏向线程ID,其他的线程如果想获取锁的话需要通过不断CAS来获取锁。如果线程的锁竞争和激烈，则可以通过JVM参数关闭，使其上升为轻量级锁。

**轻量级锁**

竞争的线程不会阻塞，提高了响应速度。追求响应时间，同步块执行的速度非常快

获取到锁时会使用CAS将对象头的Mark Word替换为指向锁记录的指针，如果其他线程再想获取锁，获取不到则进行CAS自旋操作，争抢不到则会进入重量级锁

**重量级锁**

不会使用自旋，不会消耗CPU,，但是线程阻塞，会降低响应时间。适用于追求吞吐量的场景

因为自旋会消耗CPU，所以为了避免无用的自旋，升级为重量级锁后会不会再降级，会阻塞其他线程到锁释放并被唤醒，这样有利于减少性能开销

#### JUC

Java.util.concurrent 是在并发编程中比较常用的工具类，里面包含很多用来在并发场景中使用的组件。比如线程池、阻塞队列、计时器、同步器、并发集合等等。

##### Lock

在 Lock 接口出现之前，Java 中的应用程序对于多线程的并发安全处理只能基于synchronized 关键字来解决。但是 synchronized 在有些场景中会存在一些短板，也就是它并不适合于所有的并发场景。但是在 Java5 以后，Lock 的出现可以解决synchronized 在某些场景中的短板，它比 synchronized 更加灵活。

Lock 本质上是一个接口，它定义了释放锁和获得锁的抽象方法，定义成接口就意味着它定义了锁的一个标准范，也同时意味着锁的不同实现。实现 Lock 接口的类有很多，以下为几个常见的锁实现:

* ReentrantLock：表示重入锁，它是唯一一个实现了 Lock 接口的类。**重入锁指的是线程在获得锁之后，再次获取该锁不需要阻塞**，而是直接关联一次计数器增加重入次数.
* ReentrantReadWriteLock：重入读写锁，它实现了 ReadWriteLock 接口，在这个类中维护了两个锁，一个是 ReadLock，一个是 WriteLock，他们都分别实现了 Lock接口。读写锁是一种适合读多写少的场景下解决线程安全问题的工具，基本原则是： **读和读不互斥、读和写互斥、写和写互斥**。也就是说涉及到影响数据变化的操作都会存在互斥。
* StampedLock： stampedLock 是 JDK8 引入的新的锁机制，可以简单认为是读写锁的一个改进版本，读写锁虽然通过分离读和写的功能使得读和读之间可以完全并发，但是读和写是有冲突的，如果大量的读线程存在，可能会引起写线程的饥饿。stampedLock 是一种乐观的读策略，使得乐观锁完全不会阻塞写线程。

##### Lock中的方法

* void lock()  如果锁可用就获得锁，如果锁不可用就阻塞直到锁释放
* void lockInterruptibly() 和lock()方法相似, 但阻塞的线程 可 中 断 ， 抛 出java.lang.InterruptedException 异常
* boolean tryLock()  非阻塞获取锁;尝试获取锁，如果成功返回 true
* boolean  tryLock(longtimeout, TimeUnit timeUnit) 带有超时时间的获取锁方法
* void unlock()  释放锁

##### ReentrantLock  重入锁

类图如下:

![](D:\图片\学习\JUC\ReentrantLock类图.png)

**重入锁**，表示支持重新进入的锁，也就是说，如果当前线程 t1 通过调用 lock 方法获取了锁之后，再次调用 lock，是不会再阻塞去获取锁的，直接增加重试次数就行了。synchronized 和 ReentrantLock 都是可重入锁。很多同学不理解为什么锁会存在重入的特性，那是因为对于同步锁的理解程度还不够，比如在下面这类的场景中，存在多个加锁的方法的相互调用，其实就是一种重入特性的场景。

**重入锁的设计目的**

先看一段代码

```java
public class ReentrantDemo {
    public synchronized void demo(){
        System.out.println("begin:demo");
        demo2();
    }
    public void demo2(){
        System.out.println("begin:demo1");
        synchronized (this){
        }
    }
    public static void main(String[] args) {
        ReentrantDemo rd=new ReentrantDemo();
        new Thread(rd::demo).start();
    }
}
```

调用 demo 方法获得了当前的对象锁，然后在这个方法中再去调用demo2，demo2 中的存在同一个实例锁，这个时候当前线程会因为无法获得demo2 的对象锁而阻塞，就会产生死锁。重入锁的设计目的是避免线程的死锁。

ReentrantLock 使用案例

```java
public class AtomicDemo {
private static int count=0;
static Lock lock=new ReentrantLock();
public static void inc(){
lock.lock();
try {
Thread.sleep(1);
} catch (InterruptedException e) {
e.printStackTrace();
}
count++;
lock.unlock();
}
public static void main(String[] args) throws
InterruptedException {
for(int i=0;i<1000;i++){
new Thread(()->{AtomicDemo.inc();}).start();;
}
Thread.sleep(3000);
System.out.println("result:"+count);
}
}
```

##### ReentrantReadWriteLock

我们以前理解的锁，基本都是排他锁，也就是这些锁在同一时刻只允许一个线程进行访问，而读写所在同一时刻可以允许多个线程访问，但是在写线程访问时，所有的读线程和其他写线程都会被阻塞。读写锁维护了一对锁，一个读锁、一个写锁;一般情况下，读写锁的性能都会比排它锁好，因为大多数场景读是多于写的。在读多于写的情况下，读写锁能够提供比排它锁更好的并发性和吞吐量.

```java
public class ReentrantReadWriteLockDemo {
    static Map<String,Object> cacheMap=new HashMap<>();
    static ReentrantReadWriteLock rwl=new
        ReentrantReadWriteLock();
    static Lock read=rwl.readLock();
    static Lock write=rwl.writeLock();
    public static final Object get(String key) {
        System.out.println(" 开始读取数据");
        read.lock(); // 读锁
        try {
            return cacheMap.get(key);
        }finally {
            read.unlock();
        }
    }
    public static final Object put(String key,Object value){
        write.lock();
        System.out.println(" 开始写数据");
        try{
            return cacheMap.put(key,value);
        }finally {
            write.unlock();
        }
    }
}
```

在这个案例中，通过 hashmap 来模拟了一个内存缓存，然后使用读写所来保证这个内存缓存的线程安全性。当执行读操作的时候，需要获取读锁，在并发访问的时候，读锁不会被阻塞，因为读操作不会影响执行结果。在执行写操作时，线程必须要获取写锁，当已经有线程持有写锁的情况下，当前线程会被阻塞，只有当写锁释放以后，其他读写操作才能继续执行。使用读写锁提升读操作的并发性，也保证每次写操作对所有的读写操作的可见性。

* 读锁与读锁可以共享
* 读锁与写锁不可以共享（排他）
* 写锁与写锁不可以共享（排他）

#### AQS原理

在 Lock 中，用到了一个同步队列 AQS，全称 AbstractQueuedSynchronizer，它是一个同步工具也是 Lock 用来实现线程同步的核心组件。

从使用层面来说，AQS 的功能分为两种：**独占和共享**

* 独占锁，每次只能有一个线程持有锁，比如前面给大家演示的 ReentrantLock 就是以独占方式实现的互斥锁
* 共享锁，允许多个线程同时获取锁 ，并发访问共享资源，比 如ReentrantReadWriteLock

##### AQS 内部实现原理

AQS 队列内部维护的是一个 **FIFO 的双向链表**，这种结构的特点是**每个数据结构都有两个指针**，分别指向**直接的后继节点**和**直接前驱节点**。所以双向链表可以从**任意一个节点**开始很方便的**访问前驱和后继**。**每个Node**其实是**由线程封装**，当线程**争抢锁失败**后会**封装成 Node 加入到 ASQ 队列中去**；**当获取锁的线程释放锁以后，会从队列中唤醒一个阻塞的节点(线程)。**

##### 释放锁以及添加线程对于队列的变化

当出现锁竞争以及释放锁的时候，AQS 同步队列中的节点会发生变化，首先看一下添加节点的场景。

![](C:\Users\Administrator\IdeaProjects\zhangrui\picture\juc\AQS添加新节点.png)

这里会涉及到两个变化

1. 新的线程封装成 Node 节点追加到同步队列中，设置 prev 节点以及修改当前节点的前置节点的 next 节点指向自己
2. 通过 CAS 讲 tail 重新指向新的尾部节点

head 节点表示获取锁成功的节点，当头结点在释放同步状态时，会唤醒后继节点，如果后继节点获得锁成功，会把自己设置为头结点，节点的变化过程如下:

![](C:\Users\Administrator\IdeaProjects\zhangrui\picture\juc\唤醒并设置头结点.png)

这个过程也是涉及到两个变化
1. 修改 head 节点指向下一个获得锁的节点
2. 新的获得锁的节点，将 prev 的指针指向 null

**设置 head 节点不需要用 CAS，原因是设置 head 节点是由获得锁的线程来完成的，而同步锁只能由一个线程获得，所以不需要 CAS 保证，只需要把 head 节点设置为原首节点的后继节点，并且断开原 head 节点的 next 引用即可**

##### ReentrantLock 的源码分析

ReentrantLock 时序图



##### Condition

Condition 是一个多线程协调通信的工具类，可以让某些线程一起等待某个条件（condition），只有满足条件时，线程才会被唤醒。类似于**wait**和**notify**。比如可用来实现生产者消费者模型



#### 多线程并发工具类



#### ConcurrentHashMap原理

#### 阻塞队列及原子操作

#### 线程池原理

##### 线程池配置
一个Java应用应该配置几个线程池？
是配置一个全局的线程池还是分类:每个Service或者说是每种任务分别配置一个线程池？

### JVM

JVM架构图
https://github.com/HelodieJaqueline/zhangrui/blob/master/JVM%E6%9E%B6%E6%9E%84%E5%9B%BE.png



#### 运行时数据区域

##### 程序计数器

* 程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。
* 为了线程切换后能**恢复到正确的执行位置**，**每条线程**都需要有一个**独立**的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“**线程私有**”的内存。
* 程序计数器是**唯一一个不会出现 OutOfMemoryError 的内存区域**，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。

##### Java虚拟机栈

*  与程序计数器一样，Java 虚拟机栈也是**线程私有**的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。

* **Java 内存可以粗糙的区分为堆内存（Heap）和栈内存 (Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。** （实际上，Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。）

* **局部变量表主要存放了编译器可知的各种数据类型**（boolean、byte、char、short、int、float、long、double）、**对象引用**（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。

  **Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。**

  1. **tackOverFlowError：** 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 异常。
  2. **OutOfMemoryError：** 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出 OutOfMemoryError 异常。

##### 本地方法栈

* 和虚拟机栈所发挥的作用非常相似，区别是： **虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。** 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。
* 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。
* 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种异常。

##### 虚拟机栈中的各个部分

- 局部变量表：存放方法参数和方法内部定义的局部变量，以变量槽Slot为基本单位，一个Slot可以存放32位以内的数据类型，可重用。
- 操作数栈：先入后出，32位数据类型所占栈容量为1，64为数据类型所占栈容量为2
- 动态链接：常量池中符号引用有一部分在每次运行期间转换为直接引用，这部分称为动态链接。（一部分在类加载阶段或第一次使用时转换为直接引用—静态解析）
- 方法返回地址：方法执行后退出的两种方式：正常完成出口（执行引擎遇到任意一个返回的字节码指令）和异常完成出口（在方法执行过程中遇到异常且此异常未被处理）。两种方式都需要返回到方法被调用的位置程序才能继续执行（正常退出时调用者的PC计数器的值可以作为返回地址且栈帧中很可能保存这个计数器值；异常退出返回地址要通过异常处理器表来确定，栈帧中一般不会保存）。

##### Java堆

* Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。**此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。**
* Java 堆是垃圾收集器管理的主要区域，因此也被称作**GC 堆（Garbage Collected Heap）**.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。**进一步划分的目的是更好地回收内存，或者更快地分配内存。**

##### 方法区

* 方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 **Non-Heap（非堆）**，目的应该是与 Java 堆区分开来。

* **方法区和永久代的关系**:《Java 虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 方法区和永久代的关系很像 Java 中接口和类的关系，类实现了接口，而永久代就是 HotSpot 虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是 HotSpot 的概念，方法区是 Java 虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现，其他的虚拟机实现并没有永久代这一说法。

* 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。

  JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。

**为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢?**

整个永久代有一个 JVM 本身设置固定大小上限，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，并且永远不会得到 java.lang.OutOfMemoryError。你可以使用 `-XX：MaxMetaspaceSize` 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。`-XX：MetaspaceSize` 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。

##### 运行时常量池

* 运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用）

* 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。
* DK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。

##### 直接内存

**直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 异常出现。**

* JDK1.4 中新加入的 **NIO(New Input/Output) 类**，引入了一种基于**通道（Channel）** 与**缓存区（Buffer）** 的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为**避免了在 Java 堆和 Native 堆之间来回复制数据**。
* 本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。

**Java中的引用**

- 强引用：new这类引用，只要强引用在，对象永远不会被回收。
- 软引用：描述有用但非必需的对象，在内存溢出之前，会把这些对象列入回收范围内进行第二次垃圾回收。
- 弱引用：描述非必需对象，只存活到下一次垃圾回收前。
- 虚引用：不会对生存时间造成影响，不能通过虚引用获得对象实例，只是在被虚引用的对象被回收时受到一个系统通知。

#### 常用参数

**JVM参数的含义** 实例见

| **参数名称**                | **含义**                                                   | **默认值**           |                                                              |
| --------------------------- | ---------------------------------------------------------- | -------------------- | ------------------------------------------------------------ |
| -Xms                        | 初始堆大小                                                 | 物理内存的1/64(<1GB) | 默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制. |
| -Xmx                        | 最大堆大小                                                 | 物理内存的1/4(<1GB)  | 默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制 |
| -Xmn                        | 年轻代大小(1.4or lator)                                    |                      | **注意**：此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。 整个堆大小=年轻代大小 + 年老代大小 + 持久代大小. 增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8 |
| -XX:NewSize                 | 设置年轻代大小(for 1.3/1.4)                                |                      |                                                              |
| -XX:MaxNewSize              | 年轻代最大值(for 1.3/1.4)                                  |                      |                                                              |
| -XX:PermSize                | 设置持久代(perm gen)初始值                                 | 物理内存的1/64       |                                                              |
| -XX:MaxPermSize             | 设置持久代最大值                                           | 物理内存的1/4        |                                                              |
| -Xss                        | 每个线程的堆栈大小                                         |                      | JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K.更具应用的线程所需内存大小进行 调整.在相同物理内存下,减小这个值能生成更多的线程.但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000左右 一般小的应用， 如果栈不是很深， 应该是128k够用的 大的应用建议使用256k。这个选项对性能影响比较大，需要严格的测试。（校长） 和threadstacksize选项解释很类似,官方文档似乎没有解释,在论坛中有这样一句话:"” -Xss is translated in a VM flag named ThreadStackSize” 一般设置这个值就可以了。 |
| -*XX:ThreadStackSize*       | Thread Stack Size                                          |                      | (0 means use default stack size) [Sparc: 512; Solaris x86: 320 (was 256 prior in 5.0 and earlier); Sparc 64 bit: 1024; Linux amd64: 1024 (was 0 in 5.0 and earlier); all others 0.] |
| -XX:NewRatio                | 年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代) |                      | -XX:NewRatio=4表示年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1/5 Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。 |
| -XX:SurvivorRatio           | Eden区与Survivor区的大小比值                               |                      | 设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10 |
| -XX:LargePageSizeInBytes    | 内存页的大小不可设置过大， 会影响Perm的大小                |                      | =128m                                                        |
| -XX:+UseFastAccessorMethods | 原始类型的快速优化                                         |                      |                                                              |
| -XX:+DisableExplicitGC      | 关闭System.gc()                                            |                      | 这个参数需要严格的测试                                       |
| -XX:MaxTenuringThreshold    | 垃圾最大年龄                                               |                      | 如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代. 对于年老代比较多的应用,可以提高效率.如果将此值设置为一个较大值,则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活 时间,增加在年轻代即被回收的概率 该参数只有在串行GC时才有效. |
| -XX:+AggressiveOpts         | 加快编译                                                   |                      |                                                              |
| -XX:+UseBiasedLocking       | 锁机制的性能改善                                           |                      |                                                              |
| -Xnoclassgc                 | 禁用垃圾回收                                               |                      |                                                              |
| -XX:SoftRefLRUPolicyMSPerMB | 每兆堆空闲空间中SoftReference的存活时间                    | 1s                   | softly reachable objects will remain alive for some amount of time after the last time they were referenced. The default value is one second of lifetime per free megabyte in the heap |
| -XX:PretenureSizeThreshold  | 对象超过多大是直接在旧生代分配                             | 0                    | 单位字节 新生代采用Parallel Scavenge GC时无效 另一种直接在旧生代分配的情况是大的数组对象,且数组中无外部引用对象. |
| -XX:TLABWasteTargetPercent  | TLAB占eden区的百分比                                       | 1%                   |                                                              |
| -XX:+*CollectGen0First*     | FullGC时是否先YGC                                          | false                |                                                              |

**并行收集器相关参数**

| -XX:+UseParallelGC          | Full GC采用parallel MSC (此项待验证)              |      | 选择垃圾收集器为并行收集器.此配置仅对年轻代有效.即上述配置下,年轻代使用并发收集,而年老代仍旧使用串行收集.(此项待验证) |
| --------------------------- | ------------------------------------------------- | ---- | ------------------------------------------------------------ |
| -XX:+UseParNewGC            | 设置年轻代为并行收集                              |      | 可与CMS收集同时使用 JDK5.0以上,JVM会根据系统配置自行设置,所以无需再设置此值 |
| -XX:ParallelGCThreads       | 并行收集器的线程数                                |      | 此值最好配置与处理器数目相等 同样适用于CMS                   |
| -XX:+UseParallelOldGC       | 年老代垃圾收集方式为并行收集(Parallel Compacting) |      | 这个是JAVA 6出现的参数选项                                   |
| -XX:MaxGCPauseMillis        | 每次年轻代垃圾回收的最长时间(最大暂停时间)        |      | 如果无法满足此时间,JVM会自动调整年轻代大小,以满足此值.       |
| -XX:+UseAdaptiveSizePolicy  | 自动选择年轻代区大小和相应的Survivor区比例        |      | 设置此选项后,并行收集器会自动选择年轻代区大小和相应的Survivor区比例,以达到目标系统规定的最低相应时间或者收集频率等,此值建议使用并行收集器时,一直打开. |
| -XX:GCTimeRatio             | 设置垃圾回收时间占程序运行时间的百分比            |      | 公式为1/(1+n)                                                |
| -XX:+*ScavengeBeforeFullGC* | Full GC前调用YGC                                  | true | Do young generation GC prior to a full GC. (Introduced in 1.4.1.) |

**CMS相关参数**

| -XX:+UseConcMarkSweepGC                | 使用CMS内存收集                           |      | 测试中配置这个以后,-XX:NewRatio=4的配置失效了,原因不明.所以,此时年轻代大小最好用-Xmn设置.??? |
| -------------------------------------- | ----------------------------------------- | ---- | ------------------------------------------------------------ |
| -XX:+AggressiveHeap                    |                                           |      | 试图是使用大量的物理内存 长时间大内存使用的优化，能检查计算资源（内存， 处理器数量） 至少需要256MB内存 大量的CPU／内存， （在1.4.1在4CPU的机器上已经显示有提升） |
| -XX:CMSFullGCsBeforeCompaction         | 多少次后进行内存压缩                      |      | 由于并发收集器不对内存空间进行压缩,整理,所以运行一段时间以后会产生"碎片",使得运行效率降低.此值设置运行多少次GC以后对内存空间进行压缩,整理. |
| -XX:+CMSParallelRemarkEnabled          | 降低标记停顿                              |      |                                                              |
| -XX+UseCMSCompactAtFullCollection      | 在FULL GC的时候， 对年老代的压缩          |      | CMS是不会移动内存的， 因此， 这个非常容易产生碎片， 导致内存不够用， 因此， 内存的压缩这个时候就会被启用。 增加这个参数是个好习惯。 可能会影响性能,但是可以消除碎片 |
| -XX:+UseCMSInitiatingOccupancyOnly     | 使用手动定义初始化定义开始CMS收集         |      | 禁止hostspot自行触发CMS GC                                   |
| -XX:CMSInitiatingOccupancyFraction=70  | 使用cms作为垃圾回收 使用70％后开始CMS收集 | 92   | 为了保证不出现promotion failed(见下面介绍)错误,该值的设置需要满足以下公式**[CMSInitiatingOccupancyFraction计算公式](https://www.cnblogs.com/redcreen/archive/2011/05/04/2037057.html#CMSInitiatingOccupancyFraction_value)** |
| -XX:CMSInitiatingPermOccupancyFraction | 设置Perm Gen使用到达多少比率时触发        | 92   |                                                              |
| -XX:+CMSIncrementalMode                | 设置为增量模式                            |      | 用于单CPU情况                                                |
| -XX:+CMSClassUnloadingEnabled          |                                           |      |                                                              |

**辅助信息**

| -XX:+PrintGC                          |                                                          |      | 输出形式:[GC 118250K->113543K(130112K), 0.0094143 secs] [Full GC 121376K->10414K(130112K), 0.0650971 secs] |
| ------------------------------------- | -------------------------------------------------------- | ---- | ------------------------------------------------------------ |
| -XX:+PrintGCDetails                   |                                                          |      | 输出形式:[GC [DefNew: 8614K->781K(9088K), 0.0123035 secs] 118250K->113543K(130112K), 0.0124633 secs] [GC [DefNew: 8614K->8614K(9088K), 0.0000665 secs][Tenured: 112761K->10414K(121024K), 0.0433488 secs] 121376K->10414K(130112K), 0.0436268 secs] |
| -XX:+PrintGCTimeStamps                |                                                          |      |                                                              |
| -XX:+PrintGC:PrintGCTimeStamps        |                                                          |      | 可与-XX:+PrintGC -XX:+PrintGCDetails混合使用 输出形式:11.851: [GC 98328K->93620K(130112K), 0.0082960 secs] |
| -XX:+PrintGCApplicationStoppedTime    | 打印垃圾回收期间程序暂停的时间.可与上面混合使用          |      | 输出形式:Total time for which application threads were stopped: 0.0468229 seconds |
| -XX:+PrintGCApplicationConcurrentTime | 打印每次垃圾回收前,程序未中断的执行时间.可与上面混合使用 |      | 输出形式:Application time: 0.5291524 seconds                 |
| -XX:+PrintHeapAtGC                    | 打印GC前后的详细堆栈信息                                 |      |                                                              |
| -Xloggc:filename                      | 把相关日志信息记录到文件以便分析. 与上面几个配合使用     |      |                                                              |
| -XX:+PrintClassHistogram              | garbage collects before printing the histogram.          |      |                                                              |
| -XX:+PrintTLAB                        | 查看TLAB空间的使用情况                                   |      |                                                              |
| XX:+PrintTenuringDistribution         | 查看每次minor GC后新的存活周期的阈值                     |      | Desired survivor size 1048576 bytes, new threshold 7 (max 15) new threshold 7即标识新的存活周期的阈值为7。 |



#### GC

##### GC Root:

* 类加载器
* Thread
* 虚拟机栈的本地变量表
* static成员
* 常量引用
* 本地方法
* 栈的变量等。

##### GC类型

##### GC算法

* 标记-清除(Mark-Sweep)

  缺点:

  标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程
  序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。
  (1)标记和清除两个过程都比较耗时，效率不高
  (2)会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无
  法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。

* 复制(Copying)

  空间利用率降低。

* 标记-整理(Mark-Compact)

  将存活的对象移动到另一端，移动过程中代价比较大

##### 分代收集算法

* Young区：复制算法(对象在被分配之后，可能生命周期比较短，Young区复制效率比较高)
* Old区：标记清除或标记整理(Old区对象存活时间比较长，复制来复制去没必要，不如做个标记再清理)

##### Minor GC和Full GC

- Minor GC：从新生代回收内存，关键是Eden区内存不足，造成不足的原因是Java对象大部分是朝生夕死(java局部对象)，而死掉的对象就需要在合适的时机被JVM回收
- Major GC：从老年代回收内存，一般比Minor GC慢10倍以上。
- Full GC：对整个堆来说的，出现Full GC通常伴随至少一次Minor GC，但非绝对。Full GC被触发的时候：老年代内存不足；持久代内存不足；统计得到的Minor GC晋升到老年代平均大小大于老年代空间。

##### Java虚拟机new一个对象的创建过程

- 在常量池中查看是否有new的参数对应的类的符号引用，并检查这个符号引用对应的类是否被加载、解析、初始化
- 加载后，为新对象分配内存空间，对象多需要的内存大小在类被加载之后就被确定（堆内分配内存：指针碰撞、空闲列表）。
- 将分配的空间初始化为零值。
- 对对象头进行必要设置（实例是哪个类的实例、类的元信息数据、GC分代年龄等）。
- 执行方法，按照程序的值初始化。



##### 垃圾收集器

#### 类加载

类加载过程主要包含加载、验证、准备、解析、初始化、使用、卸载七个方面

**一、加载**

在加载阶段，虚拟机主要完成三件事：

- 通过一个类的全限定名来获取定义此类的二进制字节流。
- 将这个字节流所代表的静态存储结构转化为方法区域的运行时数据结构。
- 在Java堆中生成一个代表这个类的java.lang.Class对象，作为方法区域数据的访问入口

**二、验证**

　　验证阶段作用是保证Class文件的字节流包含的信息符合JVM规范，不会给JVM造成危害。如果验证失败，就会抛出一个java.lang.VerifyError异常或其子类异常。验证过程分为四个阶段

　　1.文件格式验证：验证字节流文件是否符合Class文件格式的规范，并且能被当前虚拟机正确的处理。

　　2.元数据验证：是对字节码描述的信息进行语义分析，以保证其描述的信息符合Java语言的规范。

　　3.字节码验证：主要是进行数据流和控制流的分析，保证被校验类的方法在运行时不会危害虚拟机。

　　4.符号引用验证：符号引用验证发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在解析阶段中发生。

**三、准备**

　　准备阶段为变量分配内存并设置类变量的初始化。在这个阶段分配的仅为类的变量(static修饰的变量)，而不包括类的实例变量。对已非final的变量，JVM会将其设置成“零值”，而不是其赋值语句的值：

　　pirvate static int size = 12;

　　那么在这个阶段，size的值为0，而不是12。 final修饰的类变量将会赋值成真实的值。

**四、解析**

　　解析过程是将常量池内的符号引用替换成直接引用。主要包括四种类型引用的解析。类或接口的解析、字段解析、方法解析、接口方法解析。

**五、初始化**

　　在准备阶段，类变量已经经过一次初始化了，在这个阶段，则是根据程序员通过程序制定的计划去初始化类的变量和其他资源。这些资源有static{}块，构造函数，父类的初始化等。

##### 类加载器

* 启动类加载器：负责加载**JRE**的核心类库，如**jre**目标下的**rt**.jar,**charsets**.jar等

* 扩展类加载器：负责加载**JRE**扩展目录**ext**中**JAR**类包

* 系统类加载器：负责加载**ClassPath**路径下的类包

* 用户自定义加载器：负责加载用户自定义路径下的类包

##### 双亲委派模型

如果一个类加载器收到类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器完成。 每个类加载器都是如此，只有当父加载器在自己的搜索范围内找不到指定的类时（即ClassNotFoundException），子加载器才会尝试自己去加载。

**为什么需要双亲委派模型？**

在这里，先想一下，如果没有双亲委派，那么用户是不是可以 自己定义一个java.lang.Object的同名类 ， java.lang.String的同名类 ，并把它放到ClassPath中,那么 类之间的比较结果及类的唯一性将无法保证 ，因此，为什么需要双亲委派模型？ 防止内存中出现多份同样的字节码

**怎么打破双亲委派模型？**

打破双亲委派机制则不仅 要继承ClassLoader 类，还要 重写loadClass和findClass 方法。

##### 一个类里面变量、静态块，方法的加载顺序

父类静态变量、父类静态代码块、子类静态变量、子类静态代码块、父类非静态变量、父类非静态代码块、父类构造函数、子类非静态变量、子类非静态代码块、子类构造函数。

#### 调优

##### Linux命令排查

- 使用**top**命令查看系统资源占用高的进程

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                             
   4457 root      20   0 2458m 138m  13m S 98.5  7.4   0:33.72 java                                                                                                 
    1 root      20   0 19232  952  768 S  0.0  0.0   0:07.02 init     

- 使用 top -Hp 4457 命令

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                             
   4520 root      20   0 2458m 141m  13m R 99.2  7.5   4:40.70 java                                                                                                 
   4457 root      20   0 2458m 141m  13m S  0.0  7.5   0:00.00 java  

  - 将线程id 4520转化为16进制:11a8

- 

##### Arthas

<https://github.com/alibaba/arthas>

##### Jprofile

#### JVM常见问题

##### 为什么要分为Eden和Survivor?为什么要设置两个Survivor区？
* 如果没有Survivor，Eden区每进行一次Minor GC，存活的对象就会被送到老年代。 老年代很快被填满，触发Major GC.老年代的内存空间远大于新生代，进行一次Full GC消耗的时间比Minor GC长得多,所以需要分为Eden和Survivor。
  Survivor的存在意义，就是减少被送到老年代的对象，进而减少Full GC的发生，Survivor的预筛选保证，只有经历16次Minor GC还能在新生代中存活的对象，才会被送到老年代。
* 设置两个Survivor区最大的好处就是解决了碎片化，刚刚新建的对象在Eden中，经历一次Minor GC，Eden中的存活对象就会被移动到第一块survivor space S0，Eden被清空； 等Eden区再满了，就再触发一次Minor GC，Eden和S0中的存活对象又会被复制送入第二块survivor space S1。如果只有一块Survivor,之前存放存活对象的那块再下一次Minor GC的时候也有对象需要清除，只能用标记清除了，那么就带来内存碎片化的问题了。（这个过程非常重要，因为这种复制算法保证了S1中来自S0和Eden两部分的存活对象占用连续的内存空间，避免了碎片化的发生）

##### JVM中一次完整的GC流程是怎样的，对象如何晋升到老年代

当 Eden 区的空间满了， Java虚拟机会触发一次 Minor GC，以收集新生代的垃圾，存活下来的对象，则会转移到 Survivor区。

大对象（需要大量连续内存空间的Java对象，如那种很长的字符串） 直接进入老年态 

如果对象在Eden出生，并经过第一次Minor GC后仍然存活，并且被Survivor容纳的话，年龄设为1，每熬过一次Minor GC，年龄+1， 若年龄超过一定限制（15），则被晋升到老年态 。即 长期存活的对象进入老年态 。
动态对象年龄:如果Survivor空间中相同年龄所有对象大小之和大于Suvivor空间的一半，年龄大于前者年龄的对象会直接进入老年代。

老年代满了而 无法容纳更多的对象 ，Minor GC 之后通常就会进行Full GC，Full GC 清理整个堆内存– 包括年轻代和年老代 。

Major GC 发生在老年代的GC ， 清理老年区 ，经常会伴随至少一次Minor GC， 比Minor GC慢10倍以上 。

##### 你知道哪几种垃圾收集器，各自的优缺点，重点讲下cms和G1，包括原理，流程，优缺点。

几种垃圾收集器：

| 名称              | 简介                                                         |
| ----------------- | ------------------------------------------------------------ |
| Serial            | 单线程的收集器，收集垃圾时，必须stop the world，使用复制算法。 |
| ParNew            | Serial收集器的多线程版本，也需要stop the world，复制算法。   |
| Parallel Scavenge | 新生代收集器，复制算法的收集器，并发的多线程收集器，目标是达到一个可控的吞吐量。 如果虚拟机总共运行100分钟，其中垃圾花掉1分钟，吞吐量就是99%。 |
| Serial Old        | 是Serial收集器的老年代版本，单线程收集器，使用标记整理算法。 |
| Parallel Old      | 是Parallel Scavenge收集器的老年代版本，使用多线程，标记-整理算法。 |
| CMS               | 是一种以获得最短回收停顿时间为目标的收集器， 标记清除算法，运作过程：初始标记，并发标记，重新标记，并发清除 ，收集结束会产生大量空间碎片。 |
| G1                | 标记整理算法实现， 运作流程主要包括以下：初始标记，并发标记，最终标记，筛选标记 。 不会产生空间碎片，可以精确地控制停顿。 |

##### G1是怎么做到控制垃圾回收停顿时间的

G1会根据开发设定的预期系统停顿时间，来选择最少回收时间和最多垃圾对象的Region进行回收，效率和成果最大化，保证GC时的停顿时间在可控范围内

##### CMS收集器和G1收集器的区别

CMS收集器是老年代的收集器，可以配合新生代的Serial和ParNew收集器一起使用。G1收集器收集范围是老年代和新生代，不需要结合其他收集器使用；
CMS收集器以最小的停顿时间为目标的收集器。G1收集器可预测垃圾回收的停顿时间;
CMS收集器是使用“标记-清除”算法进行的垃圾回收，容易产生内存碎片。G1收集器使用的是“标记-整理”算法，进行了空间整合，降低了内存空间碎片。

##### 简单说说你了解的类加载器，可以打破双亲委派么，怎么打破。

**类加载器**:类加载器就是根据指定全限定名称将class文件加载到JVM内存，转为Class对象。

**启动类加载器**（Bootstrap ClassLoader）： 由C++语言实现（针对HotSpot）,负责将存放在lib目录或-Xbootclasspath参数指定的路径中的类库加载到内存中。
**其他类加载器**： 由Java语言实现，继承自抽象类ClassLoader。 如：
**扩展类加载器**（Extension ClassLoader）： 负责加载libext目录或java.ext.dirs系统变量指定的路径中的所有类库。
**应用程序类加载器**（Application ClassLoader）。 负责加载用户类路径（classpath）上的指定类库，我们可以直接使用这个类加载器。 一般情况，如果我们没有自定义类加载器默认就是用这个加载器。

##### 垃圾回收对象时程序的逻辑是否可以继续执行

不同回收器不同：Serial、ParNew会暂停用户所有线程工作；CMS、G1会在某一阶段暂停用户线程。

##### 内存分配策略

- 对象优先在Eden分配：若Eden无空间，Java虚拟机发起一次Minor GC。
- 大对象直接进入老年代：大对象指需要大量连续内存空间的对象（如长数组、长字符串）
- 长期存活的对象进入老年代：每个对象有一个对象年龄计数器，age=15晋升为老年代。age+1的两个情况：对象在Eden出生并经过一次Minor GC存活且被survivor容纳；在survivor区经历过一次minor GC。

##### 空间分配担保

- 在Minor GC之前，先检查老年代最大可用连续空间是否大于新生代所有空间总和，成立则此次GC安全
- 不成立，查看是否允许担保失败设置为true，不允许则进行Full GC
- 允许，看老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，不成立则Full GC
- 成立，则进行Minor GC

##### java中方法区存放哪些东西？jvm如何控制方法区的大小以及内存溢出的原因和解决

方法区大小不是固定的，jvm可根据需要动态调整。方法区主要存放类信息、常量、静态变量、编译后的代码。

控制方法区大小：减少程序中class数量、尽量使用较少的静态变量，减少动态代理等操作

修改：-XX:MaxPerSize调大(1.8以后是元数据区，和直接内存有关)

StackOverflowError异常：线程的方法嵌套调用层次太多，随着Java栈中桢的增多，最终会由于该线程Java栈中所有栈帧总和大于-Xss设置的值而产生此异常。

##### 可以作为GC Root的对象：

1. 虚拟机栈中引用的对象
2. 方法区中类静态属性引用的对象
3. 方法区中常量引用的对象
4. 本地方法栈中Native方法引用的对象

### 

### Java 8

### 设计模式

#### 单例模式

#### 工厂模式

#### 代理模式

#### 模板模式

#### 策略模式

#### 观察者模式 

## 数据库

### MySQL

MySQL架构

![image-20200412221233462](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200412221233462.png)

架构分层:

![image-20200412221356232](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200412221356232.png)

#### 日志文件

##### bin log

记录在服务器层面的

redo 和 undo log记录在存储引擎方面

##### redo log

##### undo log

#### 基础SQL语句

查询设备有哪些列



##### DDL



##### DML

##### DQL

#### SQL执行流程

![image-20200412215144169](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200412215144169.png)

SQL执行流程

##### 查询流程

* 建立连接

  * 长连接
  * 短连接

  查询连接保持时间:

  ```sql
  show global variables like 'wait_timeout'; -- 非交互式超时时间，如 JDBC 程序
  show global variables like 'interactive_timeout'; -- 交互式超时时间，如数据库工具
  ```

  查看 MySQL 当前有多少个连接：

  ```sql
  show global status like 'Thread%';
  ```

  Threads_cached：缓存中的线程连接数。
  Threads_connected：当前打开的连接数。

  Threads_created：为处理连接创建的线程数。
  Threads_running：非睡眠状态的连接数，通常指并发连接数。

  SHOW PROCESSLIST：查看当前连接的状态

  MySQL 服务允许的最大连接数:
  在 5.7 版本中默认是 **151** 个，最大可以设置成 **16384（2^14）**。

  ```sql
  show variables like 'max_connections';
  ```

  通信协议：

  * Unix Socket

    不需要通过网络协议

  * TCP/IP

  MySQL 使用了半双工的通信方式？
  要么是客户端向服务端发送数据，要么是服务端向客户端发送数据，这两个动作不能
  同时发生。所以客户端发送 SQL 语句给服务端的时候，（在一次连接里面）数据是不能
  分成小块发送的，不管你的 SQL 语句有多大，都是一次性发送。

   max_allowed_packet参数可以控制每次发送包的大小

  

* 查询缓存

  ``` sql
  show variables like 'query_cache%'; -- 查询是否开启缓存
  ```

  不推荐使用，8.0已经去掉了

* 语法解析和预处理

  * 语法解析

    语法分析会对 SQL 做一些语法检查，比如单引号有没有闭合，
    然后根据 MySQL 定义的语法规则，根据 SQL 语句生成一个数据结构。这个数据结构我
    们把它叫做解析树（select_lex）。

  * 预处理

    它会检查生成的解析树，解决解析器无法解析的语义。比如，它会检查表和列名是
    否存在，检查名字和别名，保证没有歧义。预处理之后得到一个新的解析树。

* 查询优化（Query  Optimizer）与查询执行计划

  * 查询优化器

    查询优化器的目的就是根据解析树生成不同的执行计划（Execution Plan），然后选
    择一种最优的执行计划，MySQL 里面使用的是基于开销（cost）的优化器，那种执行计
    划开销最小，就用哪种。

  * 优化器可以做什么

    1、当我们对多张表进行关联查询的时候，以哪个表的数据作为基准表。
    2、有多个索引可以使用的时候，选择哪个索引。

* 存储引擎

  如果对数据一致性要求比较高，需要事务支持，可以选择 InnoDB。
  如果数据查询多更新少，对查询性能要求比较高，可以选择 MyISAM。
  如果需要一个用于查询的临时表，可以选择 Memory。

* 执行引擎

  这就是我们的执行引擎，它利用存储引擎提供的相应的 API 来完成操作。
  为什么我们修改了表的存储引擎，操作方式不需要做任何改变？因为不同功能的存
  储引擎实现的 API 是相同的。
  最后把数据返回给客户端，即使没有结果也要返回。

##### 更新流程

我们说的 update 操作其实包括了更新、插入和删除。

更新流程和查询流程有什么不同呢？
基本流程也是一致的，也就是说，它也要经过解析器、优化器的处理，最后交给执行器。
**区别就在于拿到符合条件的数据之后的操作。**

**缓冲池 Buffer Pool**

首先，InnnoDB 的数据都是放在磁盘上的，InnoDB 操作数据有一个最小的逻辑单
位，叫做页（索引页和数据页）。我们对于数据的操作，不是每次都直接操作磁盘，因
为磁盘的速度太慢了。InnoDB 使用了一种缓冲池的技术，也就是把磁盘读到的页放到一
块内存区域里面。这个内存区域就叫 Buffer Pool。

下一次读取相同的页，先判断是不是在缓冲池里面，如果是，就直接读取，不用再
次访问磁盘。
修改数据的时候，先修改缓冲池里面的页。内存的数据页和磁盘数据不一致的时候，
我们把它叫做脏页。InnoDB 里面有专门的后台线程把 Buffer Pool 的数据写入到磁盘，
每隔一段时间就一次性地把多个修改写入磁盘，这个动作就叫做刷脏。

**InnoDB  内存结构 和磁盘**

![image-20200412222622807](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200412222622807.png)



#### 索引

##### 为什么要使用索引？

* 加快读取数据的速度

##### 索引的数据结构

##### 为什么不用数组？

有序数组的等值查询和比较查询效率非常高，但是更新数据的时候会出现一个问题，可能要挪动大量的数据（改变 index），所以只适合存储静态的数据。

##### 为什么不用链表？

为了支持频繁的修改，比如插入数据，我们需要采用链表。链表的话，如果是单链表，它的查找效率还是不够高。

##### 使用二查找叉树

二叉查找树既能够实现快速查找，又能够实现快速插入

**二叉查找树的特点**

* 左子树所有的节点都小于父节点
* 右子树所有的节点都大于父节点
* 投影到平面以后，就是一个有序的线性表

**存在的问题**:

它的查找耗时是和这棵树的深度相关的，在最坏的情况下时间复杂度会退化成O(n)。

所以需要一个平衡的二叉树来实现索引

##### 平衡二叉树

* 左右子树深度差绝对值不能超过 1。

InnoDB逻辑存储结构

一个表空间最多拥有 2^32 个页，默认情况下一个页的大小为 16KB，也就是说一个
表空间最多存储 64TB 的数据。如果数据不是连续的，往已经写满的页中插入数据，会导致叶页面分裂

#### SQL调优

#### 存储引擎

#### 事务隔离级别

#### 锁机制

#### 读写分离

#### 分库分表

### Redis

#### 高性能

#### 高可用

#### 高扩展

#### Redis数据结构

#### Redis内部原理

##### LRU实现算法

#### 分布式Redis

##### Redis集群间的同步

##### Sentinel

* RAFT选举算法

##### Redis分片

* Redis代理
  * Twemproxy
  * Codis

##### Redis-Cluster

**去中心化**，只需要连接其中一个节点即可(**smart client**)

**使用CRC16算法得到Hash值后对16384取模，找到Key对于的槽位，然后找到对应的RedisGroup实例**

**扩容时槽位和RedisGroup的关系如何对应？**

使用rehash

##### 高可用

和Sentinel类似，Redis-Cluster自带主从和高可用功能

在不属于自己管理槽位的实例上操作时，需要带命令-c做Redirect,不然会报错。或者将槽位和RedisGroup对应的关系做映射并缓存起来。

不足之处:

超时转移



#### Redis使用场景

##### 分布式缓存

**缓存雪崩**

* 缓存key设置了相同的过期时间
  * 过期时间设置为随机数
  * 永不过期
  * 预更新，缓存预热(数据实时性要求不高的情况下)
  * 每次修改或者访问的时候重置失效时间

**缓存穿透**

* 访问的key对应的缓存不存在

即使在数据库找不到时，也缓存起来，过期时间非常短

* 恶意攻击

布隆过滤器(BitMap)，Guava中的工具类(**BloomFilter**)

hash+位运算，可能会出现hash碰撞



**数据库缓存双写一致性**

数据更新的时候是先操作缓存还是先操作数据库？是删除缓存还是更新缓存？

权衡:一致性更重要还是性能更重要

##### 分布式锁

各种实现方式

官方： https://redis.io/topics/distlock 

##### Redis高并发问题

#### 一致性hash算法

Hash环顺时针查找槽位(Node)

新增和删除节点可以动态添加，但是会造成数据分布不均。可以通过引入虚拟节点解决

而Redis的槽位是固定的，不需要一致性Hash算法，使用CRC16算法得到Hash值后对16384取模

### MongoDB

#### 使用场景
* 日志记录
* 快递应用、空间(位置)查找
* 游戏场景，使用 MongoDB 存储游戏用户信息，用户的装备、积分等直接以内嵌文档的形式存储，方便查询、更新
* 物流场景，使用 MongoDB 存储订单信息，订单状态在运送过程中会不断更新，以 MongoDB 内嵌数组的形式来存储，一次查询就能将订单所有的变更读取出来。
* 社交场景，使用 MongoDB 存储存储用户信息，以及用户发表的朋友圈信息，通过地理位置索引实现附近的人、地点等功能
* 物联网场景，使用 MongoDB 存储所有接入的智能设备信息，以及设备汇报的日志信息，并对这些信息进行多维度的分析
* 视频直播，使用 MongoDB 存储用户信息、礼物信息等

#### 分片

#### 副本集

参考地址 https://docs.mongodb.com/v3.6/replication/ 

## 框架源码

### Spring

#### IOC

IOC(Inversion of Control)控制反转：所谓控制反转，就是把原先我们代码里面需要实现的对象创
建、依赖的代码，反转给容器来帮忙实现。



#### DI

就是指对象是被动接受依赖类而不是自己主动去找，换句话说就是指对象不是从容器中查找它依赖的类，而是在容器实例化对象的时候主动将它依赖的类注入给它。

#### AOP

#### MVC

#### 事务
##### 事务的基本原理
Spring 事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，Spring 是无
法提供事务功能的。对于纯 JDBC 操作数据库，想要用到事务，可以按照以下步骤进行:

1. 获取连接 Connection con = DriverManager.getConnection()
2. 开启事务 con.setAutoCommit(true/false);
3. 执行 CRUD;
4. 提交事务/回滚事务 con.commit() / con.rollback();
5. 关闭连接 conn.close();

使用Spring的事务管理功能后，我们可以不再写步骤 2 和 4 的代码，而是由Spirng 自
动完成。  那么 Spring 是如何在我们书写的 CRUD 之前和之后开启事务和关闭事务的
呢？解决这个问题，也就可以从整体上理解 Spring 的事务管理实现原理了。下面简单地
介绍下，注解方式为例子
配置文件开启注解驱动，在相关的类和方法上通过注解@Transactional 标识。
Spring 在启动的时候会去解析生成相关的 bean，这时候会查看拥有相关注解的类和方
法，并且为这些类和方法生成代理，并根据@Transaction 的相关参数进行相关配置注入，
这样就在代理中为我们把相关的事务处理掉了（开启正常提交事务，异常回滚事务）。
真正的数据库层的事务提交和回滚是通过 binlog 或者 redo log 实现的。

##### Spring 事务的传播属性

所谓 spring 事务的传播属性，就是定义在存在多个事务同时存在的时候，spring 应该如
何处理这些事务的行为。这些属性在 TransactionDefinition 中定义，具体常量的解释见
下表：

| 常量名称                  | 常量解释                                                     |
| ------------------------- | ------------------------------------------------------------ |
| PROPAGATION_REQUIRED      | 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择，也是 Spring默认的事务的传播。 |
| PROPAGATION_REQUIRES_NEW  | 新建事务，如果当前存在事务，把当前事务挂起。新建的事务将和被挂起的事务没有任何关系，是两个独立的事务，外层事务失败回滚之后，不能回滚内层事务执行的结果，内层事务失败抛出异常，外层事务捕获，也可以不处理回滚操作 |
| PROPAGATION_SUPPORTS      | 支持当前事务，如果当前没有事务，就以非事务方式执行。         |
| PROPAGATION_MANDATORY     | 支持当前事务，如果当前没有事务，就抛出异常。                 |
| PROPAGATION_NOT_SUPPORTED | 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。   |
| PROPAGATION_NEVER         | 以非事务方式执行，如果当前存在事务，则抛出异常。             |
| PROPAGATION_NESTED        | 如果一个活动的事务存在，则运行在一个嵌套的事务中。如果没有活动事务，则按REQUIRED 属性执行。它使用了一个单独的事务，这个事务拥有多个可以回滚的保存点。内部事务的回滚不会对外部事务造成影响。它只对DataSourceTransactionManager 事务管理器起效。 |

##### 数据库隔离级别

| 隔离级别         | 隔离级别的值 | 导致的问题 |
| ---------------- | ------------ | ---------- |
| Read-Uncommitted | 0            | 脏读       |
| Read-Committed   | 1            | 避免脏读，存在不可重复读和幻读           |
| Repeatable-Read  | 2             | 避免脏读和不可重复读，存在幻读 |
| Serializable | 3 | 解决所有问题 |

#### Spring Bean的作用域

| 作用域类型     | 作用域(Scope)                                                |
| -------------- | ------------------------------------------------------------ |
| Singleton      | spring默认作用域,单例，整个容器只有一份。使用注册表实现(放在Map中) |
| Prototype      | 代表线程每次调用或请求这个bean都会创建一个新的实例。一般情况下，有状态的bean使用该scope。 |
| Request        | 每次http请求将会有各自的bean实例,                            |
| Session        | 在一个http session中，一个bean定义对应一个bean实例           |
| global session | 在一个全局的http session中，一个bean定义对应一个bean实例。但是，这个scope只在porlet的web应用程序中才有意义，它映射到porlet的global范围的session，如果普通的web应用使用了这个scope，容器会把它作为普通的session作用域的scope创建。 |



#### ORM

### SpringMVC

#### 配置阶段流程

#### 运行阶段流程

### MyBatis

#### 应用分析与最佳实践



#### 体系结构与工作原理

#### 插件原理及Spring集成

#### 手写Mybatis

#### 原理

#### 插件

### SpringBoot

#### 优点

#### 相应注解

### Tomcat

#### 流程

#### 分层

#### 类加载

## 分布式

### 为什么要有分布式？如何设计一个高并发高可用的系统？又会遇到哪些问题

一个高并发的系统需要:

* 系统拆分 

  **优点**

  多个数据库，分散压力；

  多个系统，虚拟化，云计算，计算能力提升，降低成本；

  业务进行物理隔离，易于升级和维护；

  快速响应需求，交付周期缩短；

  ……

  **缺点**

  

* 缓存

* 消息队列

* 数据库拆分(分库分表)

* 读写分离

* 动静分离

* CDN加速

### 高并发

 为啥会有高并发 ？随着用户量激增，数据库访问越来越频繁，数据库并发撑到两千已经很不错了，刚开始使用mysql，后来发现不断超时

### Dubbo

#### Dubbo服务注册和发现的过程

* Provider(提供者)绑定指定端口并启动服务
* 提供者连接注册中心，并发本机IP、端口、应用信息和提供服务信息发送至注册中心存储
* Consumer(消费者），连接注册中心 ，并发送应用信息、所求服务信息至注册中心
* 注册中心根据 消费 者所求服务信息匹配对应的提供者列表发送至Consumer 应用缓存。
* Consumer 在发起远程调用时基于缓存的消费者列表择其一发起调用。
* Provider 状态变更会实时通知注册中心、在由注册中心实时推送至Consumer

#### Dubbo服务的调用过程

* 服务器容器负责启动、加载、运行服务提供者
* 服务提供者在启动后就可以向注册中心暴露服务
* 服务消费者在启动后就可以向注册中心订阅想要的服务
* 注册中心向服务消费者返回服务调用列表
* 服务消费者基于软负载均衡算法调用服务提供者的服务，这个服务提供者有可能是一个服务提供者列表，调用那个服务提供者就是根据负载均衡来调用了
* 服务提供者和服务消费者定时将保存在内存中的服务调用次数和服务调用时间推送给监控中心

### 注册中心

#### Zookeeper

##### 分布式锁

##### leader选举

**Leader Latch**

参与选举的所有节点，会创建一个顺序节点，其中最小的节点会设置为 master 节点, 没抢到 Leader 的节点都监听前一个节点的删除事件，在前一个节点删除后进行重新抢主，当 master 节点手动调用 close（）方法或者 master节点挂了之后，后续的子节点会抢占 master。其中 spark 使用的就是这种方法。

**LeaderSelector**
LeaderSelector 和 Leader Latch 最的差别在于，leader可以释放领导权以后，还可以继续参与竞争。

##### 数据同步

顺序一致性(不是真正的强一致性)

如果需要保证强一致性则可以使用sync()方法



#### Nacos

#### Eureka



### 消息队列

#### 为什么使用消息队列？

什么要用消息队列这个东西？用了有什么好处&坏处？为什么选某个MQ？
* 解耦
比如用户首次登录需要调用多个系统:通知积分系统增加积分，增值赠送会员包，赠送流量包，统计系统统计激活用户数等等……如果不用MQ,新增或者减少对接系统都需要修改用户服务，耦合性太强
* 异步
登录+送积分是串行的，比如分别耗时100ms,而增加MQ后写入MQ首次登录这个消息只要10ms，那么就提高了接口的响应速度
* 削峰
高峰期请求量巨大，比如晚上7:00的时候。很多请求都是要和数据库(Mysql)交互的，数据库扛不住巨大的并发量，一般的MySQL，扛到每秒2k个请求就差不多了，即使是做了缓存。
如果使用MQ，每秒5k个请求写入MQ，A系统每秒钟最多处理2k个请求，因为MySQL每秒钟最多处理2k个。A系统从MQ中慢慢拉取请求，每秒钟就拉取2k个请求，不要超过自己每秒能处理的最大请求数量就ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。
这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。
#### 消息队列有什么优点和缺点？
##### 优点

在特殊场景下有其对应的好处，解耦、异步、削峰。

**缺点**

* 系统复杂度提高

系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了，人 ABCD 四个系统好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整，MQ 一挂，整套系统崩溃的，你不就完了？如何保证消息队列的高可用可以[点击这里查看](https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/how-to-ensure-high-availability-of-message-queues.md)。

* 系统可用性降低

硬生生加个 MQ 进来，你怎么[保证消息没有重复消费](https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/how-to-ensure-that-messages-are-not-repeatedly-consumed.md)？怎么[处理消息丢失的情况](https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/how-to-ensure-the-reliable-transmission-of-messages.md)？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。

* 一致性问题

A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。

所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，做好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了 10 倍。但是关键时刻，用，还是得用的。

#### Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么区别，以及适合哪些场景？

| 特性                     | ActiveMQ                              | RabbitMQ                                           | RocketMQ                                                     | Kafka                                                        |
| ------------------------ | ------------------------------------- | -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 单机吞吐量               | 万级，比 RocketMQ、Kafka 低一个数量级 | 同 ActiveMQ                                        | 10 万级，支撑高吞吐                                          | 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 |
| topic 数量对吞吐量的影响 |                                       |                                                    | topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic | topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 |
| 时效性                   | ms 级                                 | 微秒级，这是 RabbitMQ 的一大特点，延迟最低         | ms 级                                                        | 延迟在 ms 级以内                                             |
| 可用性                   | 高，基于主从架构实现高可用            | 同 ActiveMQ                                        | 非常高，分布式架构                                           | 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 |
| 消息可靠性               | 有较低的概率丢失数据                  | 基本不丢                                           | 经过参数优化配置，可以做到 0 丢失                            | 同 RocketMQ                                                  |
| 功能支持                 | MQ 领域的功能极其完备                 | 基于 erlang 开发，并发能力很强，性能极好，延时很低 | MQ 功能较为完善，还是分布式的，扩展性好                      | 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 |

#### MQ选型对比

|                                       | 对比项                             | RabbitMQ                | Kafka                      | RocketMQ |
| ------------------------------------- | ---------------------------------- | ----------------------- | -------------------------- | -------- |
| 基本情况                              | 所属公司/组织                      | Pivotal                 | Apache                     | Apache   |
| 开发语言                              | Erlang                             | Scala、Java             | Java                       |          |
| 默认端口                              | 5672                               | 9092                    | 10911                      |          |
| 使用                                  | 多语言支持                         | 十几种                  | 十几种                     | 4种      |
| API                                   | 完善                               | 完善                    | 非常完善                   |          |
| 与 Spring 集成                        | 支持                               | 支持                    | 支持                       |          |
| 运维管理控制台                        | 自带管理界面                       | 无                      | 自带管理界面               |          |
| 权限管理、安全机制                    | Vhost和User                        | SSL、SASL               | TLS                        |          |
| 扩展能力                              | 支持插件                           |                         |                            |          |
| 文档                                  | 详细                               | 详细                    | 案例丰富                   |          |
| 社区支持                              | 更新快，活跃                       | 更新快，活跃            | 更新快，活跃               |          |
| 商业版本                              | 阿里云                             | 阿里云                  | 阿里云                     |          |
| 部署难度                              | 依赖Erlang                         | 依赖JDK、ZK             | 依赖JDK                    |          |
| 性能                                  | 并发性                             | Erlang支持，好          | 高                         | 高       |
| 消息延迟                              | 毫秒                               | 毫秒                    | 毫秒                       |          |
| 消息吞吐量（2core4GB 内存服务器压测） | 10万级  QPS                        | 20万级  QPS             | 10万级  QPS                |          |
| 消息堆积能力                          |                                    |                         |                            |          |
| 功能                                  | 支持协议                           | AMQP、MQTT、STOMP、XMPP | 自定义协议                 | JMS      |
| 消费模型                              | Push  / Pull                       | Pull                    | Pull                       |          |
| 消息过滤                              | Topic、Direct                      | 不支持                  | tag                        |          |
| 消费历史消息                          | 不支持                             | 支持                    | 支持                       |          |
| 消息追踪（tracing）                   | 支持                               | 通过拦截器实现          | 支持                       |          |
| 消息索引                              | 不支持                             | 支持                    | 支持                       |          |
| 事务性消息                            | 支持                               | 支持                    | 支持                       |          |
| 顺序性消息                            | 不支持                             | 不支持                  | 支持                       |          |
| 消费者重试                            | 支持                               | 不支持                  | 支持                       |          |
| 死信队列                              | 支持                               | 不支持                  | 支持                       |          |
| 优先级队列                            | 支持                               | 不支持                  | 不支持                     |          |
| 延迟队列                              | 死信队列或插件                     | 不支持                  | 开源版18个等级，商业版不限 |          |
| 可用+可靠                             | 持久化                             | 内存，磁盘              | 磁盘                       | 磁盘     |
| 消息丢失概率                          | 低                                 | 低                      | 低                         |          |
| 消息重复概率                          | 无                                 | 有                      | 无                         |          |
| 高可用                                | 普通集群和镜像队列                 | 分区和副本              | 队列和副本                 |          |
| 扩展性                                | 差，需要通过HAProxy+Keepalived实现 | 分布式                  | 分布式                     |          |
| 集群间消息同步                        | 支持                               | 支持                    | 支持                       |          |

综上，各种对比之后，有如下建议：

一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了；

后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高；

不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 [Apache](https://github.com/apache/rocketmq)，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。

所以**中小型公司**，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；**大型公司**，基础架构研发实力较强，用 RocketMQ 是很好的选择。

如果是**大数据领域**的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。
#### 如何保证消息队列的高可用？
##### RabbitMQ 的高可用性

RabbitMQ 是比较有代表性的，因为是基于主从（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。
RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。
**单机模式**
单机模式，就是 Demo 级别的，一般就是你本地启动了玩玩儿的，没人生产用单机模式。
**普通集群模式（无高可用性）**
普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你创建的 queue，只会放在一个 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。

这种方式确实很麻烦，也不怎么好，**没做到所谓的分布式**，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有**数据拉取的开销**，后者导致**单实例性能瓶颈**。

而且如果那个放 queue 的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你**开启了消息持久化**，让 RabbitMQ 落地存储消息的话，**消息不一定会丢**，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。

所以这个事儿就比较尴尬了，这就**没有什么所谓的高可用性**，**这方案主要是提高吞吐量的**，就是说让集群中多个节点来服务某个 queue 的读写操作。

**镜像集群模式（高可用性）**

这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会**存在于多个实例上**，就是说，每个 RabbitMQ 节点都有这个 queue 的一个**完整镜像**，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把**消息同步**到多个实例的 queue 上。

那么**如何开启这个镜像集群模式**呢？其实很简单，RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是**镜像集群模式的策略**，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。

这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，这么玩儿，不是分布式的，就**没有扩展性可言**了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并**没有办法线性扩展**你的 queue。你想，如果这个 queue 的数据量很大，大到这个机器上的容量无法容纳了，此时该怎么办呢？

##### Kafka 的高可用性

Kafka 一个最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。

这就是**天然的分布式消息队列**，就是说一个 topic 的数据，是**分散放在多个机器上的，每个机器就放一部分数据**。

实际上 RabbmitMQ 之类的，并不是分布式消息队列，它就是传统的消息队列，只不过提供了一些集群、HA(High Availability, 高可用性) 的机制而已，因为无论怎么玩儿，RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。

Kafka 0.8 以前，是没有 HA 机制的，就是任何一个 broker 宕机了，那个 broker 上的 partition 就废了，没法写也没法读，没有什么高可用性可言。

比如说，我们假设创建了一个 topic，指定其 partition 数量是 3 个，分别在三台机器上。但是，如果第二台机器宕机了，会导致这个 topic 的 1/3 的数据就丢了，因此这个是做不到高可用的。

Kafka 0.8 以后，提供了 HA 机制，就是 replica（复制品） 副本机制。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader？很简单，**要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题**，系统复杂度太高，很容易出问题。Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。

这么搞，就有所谓的**高可用性**了，因为如果某个 broker 宕机了，没事儿，那个 broker上面的 partition 在其他机器上都有副本的。如果这个宕机的 broker 上面有某个 partition 的 leader，那么此时会从 follower 中**重新选举**一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。

**写数据**的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为）

**消费**的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。

看到这里，相信你大致明白了 Kafka 是如何保证高可用机制的了，对吧？不至于一无所知，现场还能给面试官画画图。要是遇上面试官确实是 Kafka 高手，深挖了问，那你只能说不好意思，太深入的你没研究过。

#### 如何保证消息不被重复消费？或者说，如何保证消息消费的幂等性？

**可能会有哪些重复消费的问题**？

Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，**每隔一段时间**（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。

但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset，尴尬了。重启之后，少数消息会再次消费一次。

有这么个场景。数据 1/2/3 依次进入 kafka，kafka 会给这三条数据每条分配一个 offset，代表这条数据的序号，我们就假设分配的 offset 依次是 152/153/154。消费者从 kafka 去消费的时候，也是按照这个顺序去消费。假如当消费者消费了 `offset=153` 的这条数据，刚准备去提交 offset 到 zookeeper，此时消费者进程被重启了。那么此时消费过的数据 1/2 的 offset 并没有提交，kafka 也就不知道你已经消费了 `offset=153` 这条数据。那么重启之后，消费者会找 kafka 说，嘿，哥儿们，你给我接着把上次我消费到的那个地方后面的数据继续给我传递过来。由于之前的 offset 没有提交成功，那么数据 1/2 会再次传过来，如果此时消费者没有去重的话，那么就会导致重复消费。

举个例子吧。假设你有个系统，消费一条消息就往数据库里插入一条数据，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？**但是你要是消费到第二次的时候，自己判断一下是否已经消费过了，若是就直接扔了，这样不就保留了一条数据，从而保证了数据的正确性**。

一条数据重复出现两次，**数据库里就只有一条数据，这就保证了系统的幂等性**。

幂等性，通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，**不能出错**。

所以第二个问题来了，怎么保证消息队列消费的幂等性？

其实还是得**结合业务来思考**，我这里给几个思路：

- 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。
- 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。
- 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。
- 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。

#### 如何保证消息的可靠性传输？或者说，如何处理消息丢失的问题？

##### 生产者弄丢了数据

生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。

此时可以选择用 RabbitMQ 提供的事务功能，就是生产者**发送数据之前**开启 RabbitMQ 事务`channel.txSelect`，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务`channel.txRollback`，然后重试发送消息；如果收到了消息，那么可以提交事务`channel.txCommit`。

但是问题是，RabbitMQ 事务机制（同步）一搞，基本上**吞吐量会下来，因为太耗性能**。

所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 **`confirm`** 模式，在生产者那里设置开启 `confirm` 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 `ack` 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 `nack` 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。

事务机制和 `confirm` 机制最大的不同在于，**事务机制是同步的**，你提交一个事务之后会**阻塞**在那儿，但是 `confirm` 机制是**异步**的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。

所以一般在生产者这块**避免数据丢失**，都是用 `confirm` 机制的。

##### RabbitMQ 弄丢了数据

就是 RabbitMQ 自己弄丢了数据，这个你必须**开启 RabbitMQ 的持久化**，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，**恢复之后会自动读取之前存储的数据**，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，**可能导致少量数据丢失**，但是这个概率较小。

设置持久化有**两个步骤**：

- 创建 queue 的时候将其设置为持久化
  这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。
- 第二个是发送消息的时候将消息的 `deliveryMode` 设置为 2
  就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。

必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。

注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。

所以，持久化可以跟生产者那边的 `confirm` 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 `ack` 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 `ack`，你也是可以自己重发的。

##### 消费端弄丢了数据

RabbitMQ 如果丢失了数据，主要是因为你消费的时候，**刚消费到，还没处理，结果进程挂了**，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。

这个时候得用 RabbitMQ 提供的 `ack` 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 `ack`，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 `ack` 一把。这样的话，如果你还没处理完，不就没有 `ack` 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。

##### Kafka

##### 消费端弄丢了数据

唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边**自动提交了 offset**，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。

这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要**关闭自动提交** offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是**可能会有重复消费**，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。

生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。

##### Kafka 弄丢了数据

这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。

生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。

所以此时一般是要求起码设置如下 4 个参数：

- 给 topic 设置 `replication.factor` 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。
- 在 Kafka 服务端设置 `min.insync.replicas` 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。
- 在 producer 端设置 `acks=all`：这个是要求每条数据，必须是**写入所有 replica 之后，才能认为是写成功了**。
- 在 producer 端设置 `retries=MAX`（很大很大很大的一个值，无限次重试的意思）：这个是**要求一旦写入失败，就无限重试**，卡在这里了。

我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。

##### 生产者会不会弄丢数据？

如果按照上述的思路设置了 `acks=all`，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。

#### 如何限制消息队列的消费速度

### 缓存

#### Redis

为什么要用Redis

Redis的特性

### 分布式事务

#### 数据库本地事务

#### ACID

##### A:原子性(Atomicity)

一个事务(transaction)中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 

##### C:一致性(Consistency)

事务的一致性指的是在一个事务执行之前和执行之后数据库都必须处于一致性状态。如果事务成功地完成，那么系统中所有变化将正确地应用，系统处于有效状态。如果在事务中出现错误，那么系统中的所有变化将自动地回滚，系统返回到原始状态。 

##### I:隔离性(Isolation)

指的是在并发环境中，当不同的事务同时操纵相同的数据时，每个事务都有各自的完整数据空间。由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。事务查看数据更新时，数据所处的状态要么是另一事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看到中间状态的数据。

##### D:持久性(Durability)

指的是只要事务成功结束，它对数据库所做的更新就必须永久保存下来。即使发生系统崩溃，重新启动数据库系统后，数据库还能恢复到事务成功结束时的状态。 

**而事务的ACID是通过InnoDB日志和锁来保证。**事务的隔离性是通过数据库锁的机制实现的，持久性通过redo log（重做日志）来实现，原子性和一致性通过Undo log来实现。UndoLog的原理很简单，为了满足事务的原子性，在操作任何数据之前，首先将数据备份到一个地方（这个存储数据备份的地方称为UndoLog）。然后进行数据的修改。如果出现了错误或者用户执行了ROLLBACK语句，系统可以利用Undo Log中的备份将数据恢复到事务开始之前的状态。 和Undo Log相反，RedoLog记录的是新数据的备份。在事务提交前，只要将RedoLog持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是RedoLog已经持久化。系统可以根据RedoLog的内容，将所有数据恢复到最新的状态。



#### 分布式事务





### 分布式锁

#### 基于Redis的分布式锁

##### 基于lua脚本

Redisson

#### 基于Zookeeper的分布式锁

#### 二者比较
* Redis实现的分布式锁更快，效率更高，但可靠性没有Zookeeper高
* Zookeeper实现的分布式更可靠，但是效率没有Redis高

### 分布式协调

### 分布式服务治理

#### Dubbo

##### RPC(Remote Procedure Call) 远程过程调用



### 分库分表

为什么要分库分表？那些场景适合分库分表？哪些不需要分库分表

#### 水平拆分

#### 垂直拆分

#### 分库

#### 分表

#### 代码路由

#### 中间件

MyCat

#### 



## 微服务
微服务并不是一种新思想的方法。它更像是一种思想的精炼，是一种服务化思想的最佳实践方向而已，微服务其实是在SOA思路下，
随着各个企业对于服务化治理上不断的完善，以及对软件的交付链路以及基础设施逐步成熟之下的一种自然的产物。微服务也是一
种面向服务的架构模型，只是它更强调服务的粒度。也就是服务的职责更加单一更加精炼
我们也可以把 SOA 看成是微服务的超集。也就是多个微服务可以组成一个 soa 服务

### 微服务和SOA架构的区别
微服务和 SOA 架构有什么区别。这个区别一定要从架构的发展过程来了
解。这两种架构模式，其实本质上应该是在分布式架构这条时间线上，基于服务化思想的不
断完善，以及基础设施的逐步成熟之下的一种升级。既然存在于时间线的先后，那也就意味
着，这两种架构模式所关注的点不一样
1. SOA 关注的是服务的**重用性**、以及解决企业内部的信息孤岛问题
2. 微服务关注的是**解耦**，解耦和可重用性在特定的角度来看是一样，但本质上是不同的。解
耦是降低业务之间的耦合度（也就是微服务关注的服务粒度），而可重用性关注的是服务的
复用
3. 微服务会使用**更轻量级的通信协议**，使用 Restful 风格的 API。轻量级协议可以很好的支持
跨语言，使得语言生态更加丰富
4. 微服务会更多的关注 **Devops 的持续交付**，因为服务粒度更细使得开发运维变得更加重要。
所以微服务对于容器化技术的结合更加紧密
5. **SOA 应该是微服务的超集**

### 微服务组件
#### 网关
* 在微服务实施之后，各个服务的拆分粒度很小，对于客户端来说，做一个操作可能会涉及到
后端的多个服务组件的调用，那意味着它需要频繁的发起多次访问才能进行数据聚合实现用
户的功能。
* 如果我们在所有的微服务之前增加一个网关，对于客户端来说它需要做什么功能操作直接调
用网关并且告诉网关所要做的事情即可，网关根据请求的功能对后端的多个服务的数据进行
聚合，从而减少客户端的调用频次。
* 并且，由于有了网关的聚合，我们还可以在网关层对请求进行统一鉴权和认证； 包括还可
以实现限流、请求日志统一记录、 灰度发布等等
#### 服务的通信，服务注册和服务发现
服务拆分以后就会涉及到服务的远程通信，比如 http 协议或者 rpc 协议。在满足基础通信的
基础上，如何做到服务的统一管理以及服务的动态感知，就需要涉及到服务的注册中心来实
现服务注册和发现的功能
#### 负载均衡
假设服务提供者为了扩大吞吐量，采用 10 台机器的集群部署，这个时候客户端从注册中心获
得服务以后，应该调用哪台机器呢？
所以必然有一种负载均衡的机制，来实现客户端请求的分发。

### 服务拆分

### 服务治理

#### RPC

### 注册中心

除了基本的服务注册与发现机制，从开发和运维角度，至少还要考虑如下五个方面：

* 测活：服务注册之后，如何对服务进行测活以保证服务的可用性？

* 负载均衡：当存在多个服务提供者时，如何均衡各个提供者的负载？

* 集成：在服务提供端或者调用端，如何集成注册中心？

* 运行时依赖：引入注册中心之后，对应用的运行时环境有何影响？

* 可用性：如何保证注册中心本身的可用性，特别是消除单点故障？

#### 主流注册中心产品

|                 | **Nacos**                  | **Eureka**  | **Consul**        | **CoreDNS** | **Zookeeper** |
| --------------- | -------------------------- | ----------- | ----------------- | ----------- | ------------- |
| 一致性协议      | CP+AP                      | AP          | CP                | —           | CP            |
| 健康检查        | TCP/HTTP/MYSQL/Client Beat | Client Beat | TCP/HTTP/gRPC/Cmd | —           | Keep Alive    |
| 负载均衡策略    | 权重/ metadata/Selector    | Ribbon      | Fabio             | RoundRobin  | —             |
| 雪崩保护        | 有                         | 有          | 无                | 无          | 无            |
| 自动注销实例    | 支持                       | 支持        | 不支持            | 不支持      | 支持          |
| 访问协议        | HTTP/DNS                   | HTTP        | HTTP/DNS          | DNS         | TCP           |
| 监听支持        | 支持                       | 支持        | 支持              | 不支持      | 支持          |
| 多数据中心      | 支持                       | 支持        | 支持              | 不支持      | 不支持        |
| 跨注册中心同步  | 支持                       | 不支持      | 支持              | 不支持      | 不支持        |
| SpringCloud集成 | 支持                       | 支持        | 支持              | 不支持      | 支持          |
| Dubbo集成       | 支持                       | 不支持      | 不支持            | 不支持      | 支持          |
| K8S集成         | 支持                       | 不支持      | 支持              | 支持        | 不支持        |



#### Zookeeper

#### Nacos

#### Eureka

### 配置中心

#### Zookeeper

#### Nacos



更新过程:

* 客户端发起长轮训请求
* 服务端收到请求以后，先比较服务端缓存中的数据是否相同，如果不通，则直接返回
* 如果相同，则通过schedule延迟29.5s之后再执行比较
* 为了保证当服务端在29.5s之内发生数据变化能够及时通知给客户端，服务端采用事件订阅的方式
  来监听服务端本地数据变化的事件，一旦收到事件，则触发DataChangeTask的通知，并且遍历
  allStubs队列中的ClientLongPolling,把结果写回到客户端，就完成了一次数据的推送
* 如果 DataChangeTask 任务完成了数据的 “推送” 之后，ClientLongPolling 中的调度任务又开始执
  行了怎么办呢？很简单，只要在进行 “推送” 操作之前，先将原来等待执行的调度任务取消掉就可以了，这样就防止了推送操作写完响应数据之后，调度任务又去写响应数据，这时肯定会报错的。所以，在ClientLongPolling方法中，最开始的一个步骤就是删除订阅事件



### 降级、限流、熔断

### SpringCloud

#### SpringBoot

##### 使用SpringBoot的优缺点

###### 优点

* 依赖大于配置，简化了配置，大大提升了开发效率
* 方便对外输出各种形式的服务，如 REST API、WebSocket、Web、Streaming、Tasks
* 非常简洁的安全策略集成
* 强大的开发包，支持热启动
* 自动管理依赖
* 自带应用监控
* 内置了Web容器，直接以jar包方式运行

###### 缺点

* 集成度较高，使用过程中不太容易了解底层。
* 由于有默认配置，启动时容易报错
* 版本迭代速度很快，一些模块改动很大。

spring-boot-starter的启动过程



#### 配置中心

* SpringCluod Config
* Nacos
* Apollo
* diconf
* diamond

#### 注册中心

* Nacos
* Eureka
* consul
* zookeeper

#### 路由和负载均衡

ribbon

#### 服务调用

Feign

#### 限流降级、熔断、容错 

* Sentinel
* hystrix

#### 网关

* SpringCloud Gateway
* SpringCloud  zuul

#### 链路监控

* sleuth + zipin

#### 一个配置中心的设计需要考虑哪些问题

* 读取配置源
* 合并配置源
* 定义顺序
* 存储配置
* 存储Profile
* 转换类型
* 缓存
* 动态更新

长连接、心跳

#### Spring-Cloud-Config

如何控制读取远程配置在读取本地配置之前

## DevOps相关

### Git

#### SVN与Git

##### 原理上

1. Git直接记录文件快照，SVN每次记录哪些文件作了更新、更新哪些行的内容
2. Git 有本地仓库和远程仓库，SVN没有本地仓库
3. Git 大多数操作是本地执行，SVN操作几乎都需要连接网络

##### 操作上

1. Git提交后保存在本地仓库，需要推到远程仓库；SVN提交后在远程仓库
2. Git有各种“反悔”命令，SVN几乎没有
3. Git有真正的branch，svn的branch其实是工作空间的副本

## 工具相关





## 其他

### 项目介绍

#### 项目背景

#### 承担的角色

#### 结果怎么样

#### 遇到了什么问题

#### 学到了什么





### 中台

 中台是真正为前台而生的平台（可以是技术平台，业务能力甚至是组织机构），它存在的唯一目的就是更好地服务前台规模化创新，进而更好地响应服务引领用户，使企业真正做到自身能力与用户需求的持续对接。 

 以用户为中心的持续规模化创新，是中台建设的核心目标。企业的业务响应能⼒和规模化创新能力，是互联⽹时代企业综合竞争⼒的核⼼体现。平台化包括中台化只是帮助企业达到这个目标的⼿段，并不是⽬标本身 。

 中台（⽆论是技术中台、业务中台还是组织中台）的建设，根本上是为了解决企业响应⼒困境，弥补“创新驱动快速变化”的前台和“稳定可靠驱动变化周期相对较慢”的后台之间的⽭盾，提供⼀个中间层来适配前台与后台的配速问题，链接前台需求与后台资源，帮助企业不断提升用户响应⼒。 

   因为企业后台往往并不能很好地支撑前台快速创新响应用户的需求，后台更多解决的是企业管理效率问题，而中台要解决的才是前台的创新问题。 

 所以，中台到底是什么根本不重要，如何想方设法持续提高企业对于⽤户的响应⼒才是最重要的。⽽平台化或是中台化，只是恰巧走在了这条正确的⼤道上。 

中台的优点:

* 快速响应需求，迭代迅速

原先需要一个月甚至更久的排期缩短到一周左右

* 更利于创新，提升用户满意度

期间用户投诉率下降了80%左右

* 维护成本小，风险低，更加安全
* 使更多的业务逻辑从前台转到中台

### 在工作中碰到过什么难题？怎么解决的? STAR法则（Situation Task Action Result)

一、登录接口响应非常慢，造成用户投诉

- **Situation：** 用户量激增，流量突然陡增，接口响应特别慢，前端用户登录异常
- **Task:** 为什么会慢，以前是否出现过这种情况，其他分省的项目是否有这种情况
- **Action：** 查看监控日志，接口并发量，使用Arthas分析该接口的调用执行情况,GC及内存正常。
- **Result：** 发现接口耗时都在调用一个Http接口上，查看监控发现该接口响应并不慢。进行网络抓包，发现发出该请求与服务端收到请求相隔很长时间。难道是有死锁还是什么？使用jstack将线程dump下来分析，发现很多用户线程都处于blocked状态，都在等待http工具释放连接，最后查看代码发现使用的是httpclient默认的配置，对于同一host每次只有两个连接。重新配置httpclient连接池并测试，问题解决



## 面试

### 自我介绍

我叫张锐，来自安徽，17年毕业于安徽中医药大学计算机科学与技术专业，在校期间参与过编程比赛，也获得过一些奖项。我之前就职的是一家互联网电视公司，在职期间经历了公司业务系统从单体架构到垂直应用架构、再到分布式服务架构。主要负责的是用户服务中台，包括用户管理、用户中心啊，积分系统，消息系统。工作期间我会主导一些项目的开发，落地，进度推进，线上问题排查与解决。我的情况大概就是这样，谢谢！

### 技术相关

#### 工作中用到了哪些设计模式？

模板方法模式

比如说重构登录接口，获取用户信息、权限校验这种公共方法放在父类里面，需要同步不同的第三方就放到分省去做

比如说

* 继承

  父类中定义流程、公共方法

* 多态

  子类实现有差异的地方

* 封装

  隐藏细节

策略模式

* 不同的策略类执行不通的算法，返回相同的结果

原型模式

省去了大量get和set代码

#### 从浏览器输入一个网址，敲下回车，都经历过什么？

#### System.out.printl("helloword")从编写代码到运行输入结束都经历了什么样的流程？

* 编写源代码
* 编译，将源代码编译成字节码
* 类加载器加载字节码文件，生成相应的Class对象到方法区
  * 加载
  * 验证
  * 准备
  * 解析
  * 初始化
* 执行引擎找到入口方法main()函数，执行其中的字节码指令

### 业务相关

挑一个难度最大，最有闪光点的项目说一下:有哪些技术难点，怎么解决的？给项目、公司带来了什么收益？自己有什么成长。

### 个人软实力相关

#### 平时如何学习的

#### 沟通能力

#### 有什么兴趣爱好、特长

喜欢读书和思考，好奇心，遇到不懂的会立马上网查询或者请教他人并。天文地理，投资理财



### 当面试官问“你有什么要问我的吗”时，应该说什么？

* 你在公司的一天是如何度过的？
* 能否给我简单介绍下贵公司业务与战略的未来发展
* 贵公司最让你自豪的企业文化是什么
* 团队、公司现在面临的最大挑战是什么
* 对于未来加入这个团队，你对我的期望是什么

## TODO List

Dubbo服务调用过程

